{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyNLoDuHJ4+AHmYvyj2h0e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/PremCouture/bdc96268aaec3eb66d9f3abf7650e3f1/code_to_optimize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf73VQ-bKAbS",
        "outputId": "3407d3b2-c892-47e9-bb14-b0049899320b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ENHANCED TRADING SYSTEM - COMPLETE PROPRIETARY INTEGRATION\n",
            "LIMITED TO 5 STOCKS FOR COLAB MEMORY EFFICIENCY\n",
            "============================================================\n",
            "Running in Google Colab\n",
            "Mounting Google Drive...\n",
            "Not running in Google Colab\n",
            "\n",
            "Processing 0 stocks: \n",
            "\n",
            "1. Loading stock data with ALL proprietary features...\n",
            "ERROR: No stock data loaded. Check file paths and stock IDs.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ENHANCED TRADING SYSTEM WITH FULL PROPRIETARY FEATURE INTEGRATION\n",
        "Version 5.0 - Complete implementation with data loader\n",
        "- Includes all data loading functions\n",
        "- Forces inclusion of ALL proprietary/technical features\n",
        "- Creates extensive interaction features\n",
        "- Adds non-linear transformations\n",
        "- Shows complete feature presence in all outputs\n",
        "- MODIFIED: Limited to 5 stocks for Colab memory constraints\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import os\n",
        "import sys\n",
        "import shap\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "# ==========================\n",
        "# LOGGING CONFIGURATION\n",
        "# ==========================\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==========================\n",
        "# CONFIGURATION BLOCK\n",
        "# ==========================\n",
        "\n",
        "CONFIG = {\n",
        "    # Paths\n",
        "    'STOCK_DATA_PATH': '/content/drive/MyDrive/csv_files/stock_csvs/data',\n",
        "    'FRED_ROOT_PATH': '/content/drive/MyDrive/csv_files/stock_csvs',\n",
        "\n",
        "    # Analysis parameters\n",
        "    'HORIZONS': [30, 45, 60],  # Prediction horizons in days\n",
        "    'MIN_SAMPLES_PER_TICKER': 200,  # Minimum samples required\n",
        "    'MIN_SAMPLES_FOR_TRAINING': 100,  # Minimum samples for model training\n",
        "    'MAX_STOCKS': 5,  # LIMIT TO 5 STOCKS FOR COLAB\n",
        "\n",
        "    # Feature configuration\n",
        "    'EXCLUDE_FROM_SHAP': ['USREC', 'recession', 'binary_feature'],  # Features to exclude from SHAP\n",
        "    'BINARY_VARIANCE_THRESHOLD': 0.05,  # Threshold for binary feature detection\n",
        "\n",
        "    # Model parameters - UPDATED FOR BETTER FEATURE DIVERSITY\n",
        "    'TEST_SIZE': 0.2,\n",
        "    'RANDOM_STATE': 42,\n",
        "    'N_ESTIMATORS': 150,  # Increased for better feature exploration\n",
        "    'MAX_DEPTH': 12,  # Increased depth\n",
        "    'MAX_FEATURES': 0.5,  # Use 50% of features at each split (was 'sqrt')\n",
        "    'MIN_SAMPLES_SPLIT': 30,  # Reduced to allow more splits\n",
        "    'MIN_SAMPLES_LEAF': 10,  # Reduced for more granular decisions\n",
        "\n",
        "    # Display parameters\n",
        "    'MAX_SHAP_FEATURES': 5,  # Show top 5 features\n",
        "    'CONFIDENCE_THRESHOLD': 45,\n",
        "\n",
        "    # ALL Proprietary/Technical features to ALWAYS include\n",
        "    'PROPRIETARY_FEATURES': [\n",
        "        'VIX', 'FNG', 'RSI', 'AnnVolatility', 'Momentum125',\n",
        "        'PriceStrength', 'VolumeBreadth', 'CallPut', 'NewsScore',\n",
        "        'MACD', 'BollingerBandWidth', 'ATR', 'StochRSI',\n",
        "        'OBV', 'CMF', 'ADX', 'Williams_R', 'CCI', 'MFI'\n",
        "    ],\n",
        "\n",
        "    # Regime thresholds for binary flags\n",
        "    'REGIME_THRESHOLDS': {\n",
        "        'VIX': {'extreme_high': 40, 'high': 30, 'low': 15, 'extreme_low': 10},\n",
        "        'FNG': {'extreme_high': 85, 'high': 75, 'low': 25, 'extreme_low': 15},\n",
        "        'RSI': {'extreme_high': 80, 'high': 70, 'low': 30, 'extreme_low': 20},\n",
        "        'AnnVolatility': {'extreme_high': 50, 'high': 40, 'low': 20, 'extreme_low': 15},\n",
        "        'VolumeBreadth': {'extreme_high': 2.0, 'high': 1.5, 'low': 0.5, 'extreme_low': 0.3},\n",
        "        'Momentum125': {'extreme_high': 50, 'high': 30, 'low': -10, 'extreme_low': -30},\n",
        "        'PriceStrength': {'extreme_high': 100, 'high': 50, 'low': -25, 'extreme_low': -50}\n",
        "    },\n",
        "\n",
        "    # Non-linear transformations to apply\n",
        "    'TRANSFORMATIONS': ['log', 'square', 'sqrt', 'rank']\n",
        "}\n",
        "\n",
        "# Stock to ID mapping\n",
        "STOCK_ALTERNATIVE_NAMES = {\n",
        "    'MSFT': ['49462172'], 'AMZN': ['6248713'], 'NFLX': ['265768'], 'ENPH': ['105368327'],\n",
        "    'TSLA': ['272093'], 'HPE': ['209411798'], 'META': ['270662'], 'AAPL': ['4661'],\n",
        "    'KEYS': ['170167160'], 'AMD': ['265681'], 'GOOGL': ['5552'], 'VRSN': ['4173050'],\n",
        "    'ACN': ['67889930'], 'STX': ['491932113'], 'NVDA': ['273544'], 'CDW': ['130432552'],\n",
        "    'NXPI': ['77791077'], 'FTNT': ['70236214'], 'JNPR': ['6248722'], 'SNPS': ['274499'],\n",
        "    'CRM': ['271568'], 'PYPL': ['270869'], 'FIS': ['37866641'], 'INTC': ['13096'],\n",
        "}\n",
        "\n",
        "# Create reverse mapping from ID to ticker name\n",
        "STOCK_ID_TO_NAME = {}\n",
        "for ticker, ids in STOCK_ALTERNATIVE_NAMES.items():\n",
        "    for stock_id in ids:\n",
        "        STOCK_ID_TO_NAME[stock_id] = ticker\n",
        "\n",
        "# FRED indicator metadata with lag times\n",
        "FRED_METADATA = {\n",
        "    'GDP': {\n",
        "        'name': 'Gross Domestic Product',\n",
        "        'frequency': 'Quarterly',\n",
        "        'lag_days': 45,\n",
        "        'report_window': 90,\n",
        "        'typical_release': 'Last Thursday of month after quarter end'\n",
        "    },\n",
        "    'UNRATE': {\n",
        "        'name': 'Unemployment Rate',\n",
        "        'frequency': 'Monthly',\n",
        "        'lag_days': 15,\n",
        "        'report_window': 30,\n",
        "        'typical_release': 'First Friday of following month'\n",
        "    },\n",
        "    'CPIAUCSL': {\n",
        "        'name': 'Consumer Price Index',\n",
        "        'frequency': 'Monthly',\n",
        "        'lag_days': 15,\n",
        "        'report_window': 30,\n",
        "        'typical_release': 'Mid-month for prior month'\n",
        "    },\n",
        "    'PAYEMS': {\n",
        "        'name': 'Non-Farm Payrolls',\n",
        "        'frequency': 'Monthly',\n",
        "        'lag_days': 15,\n",
        "        'report_window': 30,\n",
        "        'typical_release': 'First Friday of following month'\n",
        "    },\n",
        "    'FEDFUNDS': {\n",
        "        'name': 'Federal Funds Rate',\n",
        "        'frequency': 'Daily',\n",
        "        'lag_days': 2,\n",
        "        'report_window': 1,\n",
        "        'typical_release': 'Daily with 1-2 day lag'\n",
        "    },\n",
        "    'UMCSENT': {\n",
        "        'name': 'Consumer Sentiment',\n",
        "        'frequency': 'Monthly',\n",
        "        'lag_days': 18,\n",
        "        'report_window': 30,\n",
        "        'typical_release': 'Mid and end of month'\n",
        "    },\n",
        "    'ICSA': {\n",
        "        'name': 'Initial Jobless Claims',\n",
        "        'frequency': 'Weekly',\n",
        "        'lag_days': 5,\n",
        "        'report_window': 7,\n",
        "        'typical_release': 'Thursday for prior week'\n",
        "    },\n",
        "    'VIXCLS': {\n",
        "        'name': 'VIX Volatility Index',\n",
        "        'frequency': 'Daily',\n",
        "        'lag_days': 1,\n",
        "        'report_window': 1,\n",
        "        'typical_release': 'Real-time/Daily'\n",
        "    },\n",
        "    'DGS10': {\n",
        "        'name': '10-Year Treasury Rate',\n",
        "        'frequency': 'Daily',\n",
        "        'lag_days': 1,\n",
        "        'report_window': 1,\n",
        "        'typical_release': 'Daily'\n",
        "    },\n",
        "    'DGS2': {\n",
        "        'name': '2-Year Treasury Rate',\n",
        "        'frequency': 'Daily',\n",
        "        'lag_days': 1,\n",
        "        'report_window': 1,\n",
        "        'typical_release': 'Daily'\n",
        "    },\n",
        "    'T10Y2Y': {\n",
        "        'name': '10Y-2Y Treasury Spread',\n",
        "        'frequency': 'Daily',\n",
        "        'lag_days': 1,\n",
        "        'report_window': 1,\n",
        "        'typical_release': 'Daily'\n",
        "    },\n",
        "    'INDPRO': {\n",
        "        'name': 'Industrial Production Index',\n",
        "        'frequency': 'Monthly',\n",
        "        'lag_days': 15,\n",
        "        'report_window': 30,\n",
        "        'typical_release': 'Mid-month for prior month'\n",
        "    },\n",
        "    'HOUST': {\n",
        "        'name': 'Housing Starts',\n",
        "        'frequency': 'Monthly',\n",
        "        'lag_days': 18,\n",
        "        'report_window': 30,\n",
        "        'typical_release': 'Third week of following month'\n",
        "    },\n",
        "    'RETAILSL': {\n",
        "        'name': 'Retail Sales',\n",
        "        'frequency': 'Monthly',\n",
        "        'lag_days': 15,\n",
        "        'report_window': 30,\n",
        "        'typical_release': 'Mid-month for prior month'\n",
        "    },\n",
        "    'AMERIBOR': {\n",
        "        'name': 'American Interbank Offered Rate',\n",
        "        'frequency': 'Daily',\n",
        "        'lag_days': 2,\n",
        "        'report_window': 1,\n",
        "        'typical_release': 'Daily with 1-2 day lag'\n",
        "    },\n",
        "    'USREC': {\n",
        "        'name': 'US Recession Indicator',\n",
        "        'frequency': 'Monthly',\n",
        "        'lag_days': 60,\n",
        "        'report_window': 30,\n",
        "        'typical_release': 'NBER declaration with significant lag',\n",
        "        'is_binary': True\n",
        "    }\n",
        "}\n",
        "\n",
        "# ==========================\n",
        "# HELPER FUNCTIONS\n",
        "# ==========================\n",
        "\n",
        "def convert_np(obj):\n",
        "    \"\"\"Convert numpy types to Python types for JSON serialization\"\"\"\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, pd.Series):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, pd.Timestamp):\n",
        "        return obj.isoformat()\n",
        "    raise TypeError(f'Object of type {type(obj).__name__} is not JSON serializable')\n",
        "\n",
        "def ensure_scalar(value):\n",
        "    \"\"\"Convert numpy/pandas objects to Python scalars safely\"\"\"\n",
        "    if isinstance(value, (int, float, bool, str)) or value is None:\n",
        "        return value\n",
        "\n",
        "    if isinstance(value, np.ndarray):\n",
        "        if value.size == 1:\n",
        "            return float(value.flatten()[0])\n",
        "        elif value.size == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return float(value[0])\n",
        "\n",
        "    if isinstance(value, pd.Series):\n",
        "        if len(value) == 1:\n",
        "            return float(value.iloc[0])\n",
        "        elif len(value) == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return float(value.iloc[0])\n",
        "\n",
        "    if hasattr(value, 'item'):\n",
        "        try:\n",
        "            return value.item()\n",
        "        except ValueError:\n",
        "            return 0\n",
        "\n",
        "    try:\n",
        "        return float(value)\n",
        "    except:\n",
        "        return value\n",
        "\n",
        "def display_value(value, decimal_places=1):\n",
        "    \"\"\"Format value for display, showing '—' for missing values\"\"\"\n",
        "    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
        "        return '—'\n",
        "    elif isinstance(value, (int, np.integer)):\n",
        "        return str(int(value))\n",
        "    elif isinstance(value, (float, np.floating)):\n",
        "        if abs(value) >= 1000:\n",
        "            return f\"{value:.0f}\"\n",
        "        elif abs(value) >= 10:\n",
        "            return f\"{value:.{decimal_places}f}\"\n",
        "        elif abs(value) >= 1:\n",
        "            return f\"{value:.{decimal_places+1}f}\"\n",
        "        else:\n",
        "            return f\"{value:.{decimal_places+2}f}\"\n",
        "    else:\n",
        "        return str(value)\n",
        "\n",
        "def identify_binary_features(df, threshold=0.05):\n",
        "    \"\"\"Identify binary or low-variance features\"\"\"\n",
        "    binary_features = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
        "            unique_values = df[col].dropna().unique()\n",
        "            if len(unique_values) <= 2:\n",
        "                binary_features.append(col)\n",
        "            else:\n",
        "                # Check if variance is very low\n",
        "                if df[col].std() / (df[col].mean() + 1e-8) < threshold:\n",
        "                    binary_features.append(col)\n",
        "\n",
        "    return binary_features\n",
        "\n",
        "# ==========================\n",
        "# TECHNICAL INDICATOR CALCULATIONS\n",
        "# ==========================\n",
        "\n",
        "def calculate_rsi(prices, period=14):\n",
        "    \"\"\"Calculate RSI (Relative Strength Index)\"\"\"\n",
        "    delta = prices.diff()\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "\n",
        "    avg_gain = gain.rolling(window=period).mean()\n",
        "    avg_loss = loss.rolling(window=period).mean()\n",
        "\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    return rsi\n",
        "\n",
        "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
        "    \"\"\"Calculate MACD (Moving Average Convergence Divergence)\"\"\"\n",
        "    ema_fast = prices.ewm(span=fast).mean()\n",
        "    ema_slow = prices.ewm(span=slow).mean()\n",
        "\n",
        "    macd_line = ema_fast - ema_slow\n",
        "    signal_line = macd_line.ewm(span=signal).mean()\n",
        "    histogram = macd_line - signal_line\n",
        "\n",
        "    bullish_cross = False\n",
        "    bearish_cross = False\n",
        "    if len(macd_line) >= 2 and len(signal_line) >= 2:\n",
        "        try:\n",
        "            curr_macd = float(macd_line.iloc[-1])\n",
        "            prev_macd = float(macd_line.iloc[-2])\n",
        "            curr_signal = float(signal_line.iloc[-1])\n",
        "            prev_signal = float(signal_line.iloc[-2])\n",
        "\n",
        "            if not any(pd.isna([curr_macd, prev_macd, curr_signal, prev_signal])):\n",
        "                bullish_cross = bool(curr_macd > curr_signal and prev_macd <= prev_signal)\n",
        "                bearish_cross = bool(curr_macd < curr_signal and prev_macd >= prev_signal)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return {\n",
        "        'macd': macd_line,\n",
        "        'signal': signal_line,\n",
        "        'histogram': histogram,\n",
        "        'bullish_cross': bullish_cross,\n",
        "        'bearish_cross': bearish_cross\n",
        "    }\n",
        "\n",
        "def calculate_bollinger_bands(prices, period=20, num_std=2):\n",
        "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
        "    sma = prices.rolling(window=period).mean()\n",
        "    std = prices.rolling(window=period).std()\n",
        "\n",
        "    upper_band = sma + (std * num_std)\n",
        "    lower_band = sma - (std * num_std)\n",
        "\n",
        "    return {\n",
        "        'upper': upper_band,\n",
        "        'middle': sma,\n",
        "        'lower': lower_band,\n",
        "        'bandwidth': (upper_band - lower_band) / sma,\n",
        "        'percent_b': (prices - lower_band) / (upper_band - lower_band)\n",
        "    }\n",
        "\n",
        "def calculate_atr(high, low, close, period=14):\n",
        "    \"\"\"Calculate Average True Range\"\"\"\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift())\n",
        "    tr3 = abs(low - close.shift())\n",
        "\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    atr = tr.rolling(window=period).mean()\n",
        "\n",
        "    return atr\n",
        "\n",
        "def calculate_stochastic_rsi(rsi, period=14):\n",
        "    \"\"\"Calculate Stochastic RSI\"\"\"\n",
        "    rsi_min = rsi.rolling(window=period).min()\n",
        "    rsi_max = rsi.rolling(window=period).max()\n",
        "\n",
        "    stoch_rsi = (rsi - rsi_min) / (rsi_max - rsi_min + 1e-8) * 100\n",
        "    return stoch_rsi\n",
        "\n",
        "def calculate_obv(prices, volumes):\n",
        "    \"\"\"Calculate On Balance Volume\"\"\"\n",
        "    price_change = prices.diff()\n",
        "    obv = (volumes * np.sign(price_change)).cumsum()\n",
        "    return obv\n",
        "\n",
        "def calculate_cmf(high, low, close, volume, period=20):\n",
        "    \"\"\"Calculate Chaikin Money Flow\"\"\"\n",
        "    mf_multiplier = ((close - low) - (high - close)) / (high - low + 1e-8)\n",
        "    mf_volume = mf_multiplier * volume\n",
        "    cmf = mf_volume.rolling(period).sum() / volume.rolling(period).sum()\n",
        "    return cmf\n",
        "\n",
        "def calculate_adx(high, low, close, period=14):\n",
        "    \"\"\"Calculate Average Directional Index\"\"\"\n",
        "    plus_dm = high.diff()\n",
        "    minus_dm = low.diff().abs()\n",
        "\n",
        "    plus_dm[plus_dm < 0] = 0\n",
        "    minus_dm[minus_dm < 0] = 0\n",
        "\n",
        "    tr = calculate_atr(high, low, close, 1)\n",
        "\n",
        "    plus_di = 100 * (plus_dm.rolling(period).sum() / tr.rolling(period).sum())\n",
        "    minus_di = 100 * (minus_dm.rolling(period).sum() / tr.rolling(period).sum())\n",
        "\n",
        "    dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-8)\n",
        "    adx = dx.rolling(period).mean()\n",
        "\n",
        "    return adx\n",
        "\n",
        "def calculate_williams_r(high, low, close, period=14):\n",
        "    \"\"\"Calculate Williams %R\"\"\"\n",
        "    highest_high = high.rolling(period).max()\n",
        "    lowest_low = low.rolling(period).min()\n",
        "\n",
        "    williams_r = -100 * (highest_high - close) / (highest_high - lowest_low + 1e-8)\n",
        "    return williams_r\n",
        "\n",
        "def calculate_cci(high, low, close, period=20):\n",
        "    \"\"\"Calculate Commodity Channel Index\"\"\"\n",
        "    typical_price = (high + low + close) / 3\n",
        "    sma = typical_price.rolling(period).mean()\n",
        "    mad = typical_price.rolling(period).apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
        "\n",
        "    cci = (typical_price - sma) / (0.015 * mad + 1e-8)\n",
        "    return cci\n",
        "\n",
        "def calculate_mfi(high, low, close, volume, period=14):\n",
        "    \"\"\"Calculate Money Flow Index\"\"\"\n",
        "    typical_price = (high + low + close) / 3\n",
        "    raw_money_flow = typical_price * volume\n",
        "\n",
        "    positive_flow = raw_money_flow.where(typical_price > typical_price.shift(1), 0)\n",
        "    negative_flow = raw_money_flow.where(typical_price < typical_price.shift(1), 0)\n",
        "\n",
        "    positive_mf = positive_flow.rolling(period).sum()\n",
        "    negative_mf = negative_flow.rolling(period).sum()\n",
        "\n",
        "    mfi = 100 - (100 / (1 + positive_mf / (negative_mf + 1e-8)))\n",
        "    return mfi\n",
        "\n",
        "def calculate_momentum_125(prices):\n",
        "    \"\"\"Calculate 125-day momentum (approximately 6 months)\"\"\"\n",
        "    if len(prices) < 125:\n",
        "        return pd.Series(index=prices.index, dtype=float)\n",
        "    return prices.pct_change(125) * 100\n",
        "\n",
        "def calculate_price_strength(prices, volume=None):\n",
        "    \"\"\"Calculate price strength indicator\"\"\"\n",
        "    # Price momentum over different periods\n",
        "    mom_5 = prices.pct_change(5)\n",
        "    mom_20 = prices.pct_change(20)\n",
        "    mom_60 = prices.pct_change(60)\n",
        "\n",
        "    # Weighted average\n",
        "    price_strength = (mom_5 * 0.5 + mom_20 * 0.3 + mom_60 * 0.2) * 100\n",
        "\n",
        "    # If volume available, incorporate volume-weighted strength\n",
        "    if volume is not None and len(volume) > 0:\n",
        "        vol_ratio = volume / volume.rolling(20).mean()\n",
        "        price_strength = price_strength * vol_ratio.clip(0.5, 2.0)\n",
        "\n",
        "    return price_strength\n",
        "\n",
        "def calculate_volume_breadth(volume, prices):\n",
        "    \"\"\"Calculate volume breadth indicator\"\"\"\n",
        "    if volume is None or len(volume) == 0:\n",
        "        return pd.Series(index=prices.index, data=1.0)\n",
        "\n",
        "    # Up volume vs down volume\n",
        "    price_change = prices.diff()\n",
        "    up_volume = volume.where(price_change > 0, 0)\n",
        "    down_volume = volume.where(price_change < 0, 0)\n",
        "\n",
        "    # Rolling sums\n",
        "    up_vol_sum = up_volume.rolling(20).sum()\n",
        "    down_vol_sum = down_volume.rolling(20).sum()\n",
        "\n",
        "    # Breadth ratio\n",
        "    breadth = up_vol_sum / (down_vol_sum + 1e-8)\n",
        "    return breadth.clip(0.1, 10.0)\n",
        "\n",
        "def calculate_trend_strength(series, period):\n",
        "    \"\"\"Calculate trend strength using linear regression slope\"\"\"\n",
        "    def trend_calc(window):\n",
        "        if len(window) < period // 2:\n",
        "            return 0\n",
        "        x = np.arange(len(window))\n",
        "        y = window.values\n",
        "        if np.std(y) == 0:\n",
        "            return 0\n",
        "        try:\n",
        "            slope = np.polyfit(x, y, 1)[0]\n",
        "            return slope / (np.std(y) + 1e-6)\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    return series.rolling(period).apply(trend_calc)\n",
        "\n",
        "def calculate_performance_metrics(y_true, y_pred, returns, prediction_days):\n",
        "    \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
        "    # Basic metrics\n",
        "    accuracy = (y_true == y_pred).mean() * 100\n",
        "\n",
        "    # Direction-based returns\n",
        "    strategy_returns = returns * (2 * y_pred - 1)  # Convert 0/1 to -1/1\n",
        "\n",
        "    # Annualized metrics\n",
        "    periods_per_year = 252 / prediction_days\n",
        "\n",
        "    # Sharpe ratio\n",
        "    if len(strategy_returns) > 0 and strategy_returns.std() > 0:\n",
        "        sharpe = np.sqrt(periods_per_year) * strategy_returns.mean() / strategy_returns.std()\n",
        "    else:\n",
        "        sharpe = 0\n",
        "\n",
        "    # Win rate\n",
        "    winning_trades = strategy_returns > 0\n",
        "    win_rate = winning_trades.mean() * 100 if len(winning_trades) > 0 else 0\n",
        "\n",
        "    # Maximum drawdown\n",
        "    cumulative_returns = (1 + strategy_returns).cumprod()\n",
        "    running_max = cumulative_returns.expanding().max()\n",
        "    drawdown_series = (cumulative_returns - running_max) / running_max\n",
        "    max_drawdown = drawdown_series.min() * 100\n",
        "\n",
        "    # Annualized return\n",
        "    avg_return = strategy_returns.mean()\n",
        "    annualized_return = (1 + avg_return) ** periods_per_year - 1\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'sharpe_ratio': sharpe,\n",
        "        'win_rate': win_rate,\n",
        "        'max_drawdown': max_drawdown,\n",
        "        'annualized_return': annualized_return * 100,\n",
        "        'avg_return_per_trade': avg_return * 100\n",
        "    }\n",
        "\n",
        "# ==========================\n",
        "# DATA LOADING FUNCTIONS\n",
        "# ==========================\n",
        "\n",
        "def get_stock_id_from_ticker(ticker):\n",
        "    \"\"\"Get numeric ID for a ticker symbol\"\"\"\n",
        "    ids = STOCK_ALTERNATIVE_NAMES.get(ticker, [])\n",
        "    return ids[0] if ids else None\n",
        "\n",
        "def standardize_columns(df):\n",
        "    \"\"\"Standardize column names to expected format\"\"\"\n",
        "    column_mapping = {\n",
        "        'close': 'Close', 'open': 'Open', 'high': 'High',\n",
        "        'low': 'Low', 'volume': 'Volume', 'adj close': 'Adj Close',\n",
        "        'vix': 'VIX', 'fng': 'FNG', 'annvolatility': 'AnnVolatility',\n",
        "        'momentum125': 'Momentum125', 'pricestrength': 'PriceStrength',\n",
        "        'volumebreadth': 'VolumeBreadth', 'callput': 'CallPut',\n",
        "        'newsscore': 'NewsScore', 'rsi': 'RSI', 'macd': 'MACD',\n",
        "        'bollingerbandwidth': 'BollingerBandWidth', 'atr': 'ATR',\n",
        "        'stochrsi': 'StochRSI', 'obv': 'OBV', 'cmf': 'CMF',\n",
        "        'adx': 'ADX', 'williams_r': 'Williams_R', 'cci': 'CCI',\n",
        "        'mfi': 'MFI'\n",
        "    }\n",
        "\n",
        "    df.columns = [column_mapping.get(col.lower(), col) for col in df.columns]\n",
        "\n",
        "    required = ['Close', 'Open', 'High', 'Low', 'Volume']\n",
        "    for col in required:\n",
        "        if col not in df.columns:\n",
        "            if col == 'Volume':\n",
        "                df[col] = 1000000\n",
        "            else:\n",
        "                df[col] = df.get('Close', 100)\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_stock_data(ticker, csv_path=None):\n",
        "    \"\"\"Load individual stock data with enhanced error handling\"\"\"\n",
        "    if csv_path is None:\n",
        "        csv_path = CONFIG['STOCK_DATA_PATH']\n",
        "\n",
        "    try:\n",
        "        stock_id = get_stock_id_from_ticker(ticker)\n",
        "        file_name = f\"{stock_id}.csv\" if stock_id else f\"{ticker}.csv\"\n",
        "        file_path = os.path.join(csv_path, file_name)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Process date column\n",
        "            date_col = None\n",
        "            for col in ['Date', 'date', 'DATE', 'Datetime', 'datetime', 'timestamp']:\n",
        "                if col in df.columns:\n",
        "                    date_col = col\n",
        "                    break\n",
        "\n",
        "            if date_col:\n",
        "                df['Date'] = pd.to_datetime(df[date_col])\n",
        "                if date_col != 'Date':\n",
        "                    df = df.drop(columns=[date_col])\n",
        "            elif df.index.name and 'date' in df.index.name.lower():\n",
        "                df['Date'] = pd.to_datetime(df.index)\n",
        "                df = df.reset_index(drop=True)\n",
        "\n",
        "            df = standardize_columns(df)\n",
        "\n",
        "            # Log ALL features found\n",
        "            found_features = {feat: feat in df.columns for feat in CONFIG['PROPRIETARY_FEATURES']}\n",
        "\n",
        "            if 'Date' in df.columns:\n",
        "                df = df.sort_values('Date')\n",
        "\n",
        "                # Data sanity check\n",
        "                logger.info(f\"{ticker} - Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
        "                logger.info(f\"{ticker} - Total rows: {len(df)}\")\n",
        "                logger.info(f\"{ticker} - Proprietary features found: {[k for k,v in found_features.items() if v]}\")\n",
        "                logger.info(f\"{ticker} - Missing proprietary features: {[k for k,v in found_features.items() if not v]}\")\n",
        "\n",
        "            return df\n",
        "        else:\n",
        "            logger.warning(f\"No data file found for {ticker}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_all_stock_data(tickers, csv_path=None):\n",
        "    \"\"\"Load stock data for multiple tickers with validation\"\"\"\n",
        "    stock_data = {}\n",
        "\n",
        "    logger.info(f\"Loading {len(tickers)} stocks...\")\n",
        "\n",
        "    for ticker in tickers:\n",
        "        df = load_stock_data(ticker, csv_path)\n",
        "        if df is not None and len(df) >= CONFIG['MIN_SAMPLES_PER_TICKER']:\n",
        "            stock_data[ticker] = df\n",
        "            logger.info(f\"Loaded {ticker} ({len(df)} rows)\")\n",
        "        elif df is not None:\n",
        "            logger.warning(f\"Skipped {ticker} - insufficient data ({len(df)} rows, need {CONFIG['MIN_SAMPLES_PER_TICKER']})\")\n",
        "\n",
        "    logger.info(f\"Successfully loaded {len(stock_data)} stocks\")\n",
        "    return stock_data\n",
        "\n",
        "def load_fred_data_from_folders():\n",
        "    \"\"\"Load FRED data from folder structure\"\"\"\n",
        "    fred_data = {}\n",
        "\n",
        "    if not os.path.exists(CONFIG['FRED_ROOT_PATH']):\n",
        "        logger.warning(f\"FRED path not found: {CONFIG['FRED_ROOT_PATH']}\")\n",
        "        return fred_data\n",
        "\n",
        "    logger.info(\"Loading FRED indicators...\")\n",
        "\n",
        "    fred_patterns = list(FRED_METADATA.keys()) + ['USREC', 'AMERIBOR', 'PCEPI', 'MEHOINUSA', 'OPHNFB']\n",
        "\n",
        "    for folder in os.listdir(CONFIG['FRED_ROOT_PATH']):\n",
        "        folder_path = os.path.join(CONFIG['FRED_ROOT_PATH'], folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        folder_upper = folder.upper().replace('_1', '')\n",
        "        is_fred = any(pattern in folder_upper for pattern in fred_patterns)\n",
        "\n",
        "        if is_fred or folder.endswith('_1'):\n",
        "            csv_names = [\n",
        "                'obs._by_real-time_period.csv',\n",
        "                'obs_by_real-time_period.csv',\n",
        "                'obs.csv',\n",
        "                'data.csv',\n",
        "                f'{folder}.csv',\n",
        "                f'{folder.lower()}.csv'\n",
        "            ]\n",
        "\n",
        "            for csv_name in csv_names:\n",
        "                csv_path = os.path.join(folder_path, csv_name)\n",
        "\n",
        "                if os.path.exists(csv_path):\n",
        "                    try:\n",
        "                        df = pd.read_csv(csv_path)\n",
        "\n",
        "                        value_col = None\n",
        "                        date_col = None\n",
        "\n",
        "                        for col in df.columns:\n",
        "                            col_lower = col.lower()\n",
        "                            if any(d in col_lower for d in ['date', 'time', 'period']):\n",
        "                                date_col = col\n",
        "                                break\n",
        "\n",
        "                        folder_base = folder.replace('_1', '')\n",
        "                        if folder_base in df.columns:\n",
        "                            value_col = folder_base\n",
        "                        elif folder_base.upper() in df.columns:\n",
        "                            value_col = folder_base.upper()\n",
        "                        elif folder_base.lower() in df.columns:\n",
        "                            value_col = folder_base.lower()\n",
        "                        else:\n",
        "                            for col in df.columns:\n",
        "                                col_lower = col.lower()\n",
        "                                if col != date_col and any(v in col_lower for v in ['value', 'val', 'observation']):\n",
        "                                    value_col = col\n",
        "                                    break\n",
        "\n",
        "                        if value_col and date_col:\n",
        "                            indicator_base = folder_base.upper()\n",
        "                            if indicator_base in FRED_METADATA:\n",
        "                                indicator_name = FRED_METADATA[indicator_base]['name']\n",
        "                            else:\n",
        "                                indicator_name = folder_base\n",
        "\n",
        "                            fred_df = pd.DataFrame({\n",
        "                                'Date': pd.to_datetime(df[date_col], errors='coerce'),\n",
        "                                'Value': pd.to_numeric(df[value_col], errors='coerce')\n",
        "                            }).dropna()\n",
        "\n",
        "                            if len(fred_df) > 0:\n",
        "                                fred_data[indicator_base] = fred_df\n",
        "                                logger.info(f\"Loaded {indicator_name} ({len(fred_df)} rows)\")\n",
        "                                break\n",
        "\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "    logger.info(f\"Loaded {len(fred_data)} FRED indicators\")\n",
        "    return fred_data\n",
        "\n",
        "# ==========================\n",
        "# MACRO DATA ALIGNMENT AND MERGE\n",
        "# ==========================\n",
        "\n",
        "def fix_macro_data_alignment(fred_data):\n",
        "    \"\"\"Fix macro data alignment with proper lags to prevent look-ahead bias\"\"\"\n",
        "    aligned_fred = {}\n",
        "\n",
        "    logger.info(\"Aligning FRED data with proper lags...\")\n",
        "\n",
        "    for indicator_name, fred_df in fred_data.items():\n",
        "        if fred_df.empty:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = fred_df.copy()\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df = df.sort_values('Date')\n",
        "\n",
        "            if indicator_name in FRED_METADATA:\n",
        "                meta = FRED_METADATA[indicator_name]\n",
        "                lag_days = meta['lag_days']\n",
        "                data_type = meta['frequency']\n",
        "                report_window = meta.get('report_window', lag_days)\n",
        "            else:\n",
        "                if len(df) > 2:\n",
        "                    avg_days = df['Date'].diff().dt.days.median()\n",
        "\n",
        "                    if avg_days > 300:\n",
        "                        data_type = 'Annual'\n",
        "                        lag_days = 60\n",
        "                        report_window = 365\n",
        "                    elif avg_days > 80:\n",
        "                        data_type = 'Quarterly'\n",
        "                        lag_days = 45\n",
        "                        report_window = 90\n",
        "                    elif avg_days > 25:\n",
        "                        data_type = 'Monthly'\n",
        "                        lag_days = 15\n",
        "                        report_window = 30\n",
        "                    elif avg_days > 5:\n",
        "                        data_type = 'Weekly'\n",
        "                        lag_days = 5\n",
        "                        report_window = 7\n",
        "                    else:\n",
        "                        data_type = 'Daily'\n",
        "                        lag_days = 2\n",
        "                        report_window = 1\n",
        "                else:\n",
        "                    data_type = 'Unknown'\n",
        "                    lag_days = 30\n",
        "                    report_window = 30\n",
        "\n",
        "            safety_margin = 3\n",
        "            total_lag = lag_days + safety_margin\n",
        "            df['Date'] = df['Date'] + pd.Timedelta(days=total_lag)\n",
        "\n",
        "            df['lag_days'] = lag_days\n",
        "            df['total_lag'] = total_lag\n",
        "            df['data_type'] = data_type\n",
        "            df['report_window'] = report_window\n",
        "            df['original_date'] = df['Date'] - pd.Timedelta(days=total_lag)\n",
        "\n",
        "            lag_suffix = f\"_{data_type[0].lower()}{total_lag}\"\n",
        "            aligned_indicator_name = f\"{indicator_name}{lag_suffix}\"\n",
        "\n",
        "            aligned_fred[aligned_indicator_name] = df\n",
        "            logger.info(f\"  {indicator_name} → {aligned_indicator_name}: {data_type} data, {total_lag}d total lag applied\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error aligning {indicator_name}: {e}\")\n",
        "\n",
        "    return aligned_fred\n",
        "\n",
        "def merge_macro_with_stock(stock_data, fred_data):\n",
        "    \"\"\"Merge macro data with stock data using proper temporal alignment\"\"\"\n",
        "    logger.info(\"Merging macro features with stock data...\")\n",
        "\n",
        "    all_dates = []\n",
        "    for ticker, df in stock_data.items():\n",
        "        if 'Date' in df.columns:\n",
        "            all_dates.extend(df['Date'].tolist())\n",
        "\n",
        "    if not all_dates:\n",
        "        return stock_data, {}\n",
        "\n",
        "    all_dates = pd.to_datetime(all_dates)\n",
        "    unique_dates = sorted(list(set(all_dates)))\n",
        "\n",
        "    macro_df = pd.DataFrame({'Date': unique_dates})\n",
        "    macro_features_added = {}\n",
        "\n",
        "    for indicator_name, fred_df in fred_data.items():\n",
        "        if fred_df.empty or 'Value' not in fred_df.columns:\n",
        "            continue\n",
        "\n",
        "        col_name = f\"fred_{indicator_name}\"\n",
        "\n",
        "        fred_df = fred_df.copy()\n",
        "        fred_df = fred_df.rename(columns={'Value': col_name})\n",
        "\n",
        "        # Use forward fill for macro data\n",
        "        macro_df = pd.merge_asof(\n",
        "            macro_df.sort_values('Date'),\n",
        "            fred_df[['Date', col_name]].sort_values('Date'),\n",
        "            on='Date',\n",
        "            direction='backward'\n",
        "        )\n",
        "\n",
        "        base_indicator = indicator_name.split('_')[0].upper()\n",
        "\n",
        "        if 'lag_days' in fred_df.columns:\n",
        "            lag_days = fred_df['lag_days'].iloc[0]\n",
        "            total_lag = fred_df['total_lag'].iloc[0] if 'total_lag' in fred_df.columns else lag_days\n",
        "            data_type = fred_df['data_type'].iloc[0] if 'data_type' in fred_df.columns else 'Unknown'\n",
        "            report_window = fred_df['report_window'].iloc[0] if 'report_window' in fred_df.columns else lag_days\n",
        "        else:\n",
        "            import re\n",
        "            match = re.search(r'_([a-z])(\\d+)$', indicator_name)\n",
        "            if match:\n",
        "                freq_char, lag = match.groups()\n",
        "                total_lag = int(lag)\n",
        "                lag_days = total_lag - 3\n",
        "\n",
        "                freq_map = {'d': 'Daily', 'w': 'Weekly', 'm': 'Monthly', 'q': 'Quarterly', 'a': 'Annual'}\n",
        "                data_type = freq_map.get(freq_char, 'Unknown')\n",
        "\n",
        "                window_map = {'d': 1, 'w': 7, 'm': 30, 'q': 90, 'a': 365}\n",
        "                report_window = window_map.get(freq_char, 30)\n",
        "            else:\n",
        "                lag_days = 30\n",
        "                total_lag = 33\n",
        "                data_type = 'Unknown'\n",
        "                report_window = 30\n",
        "\n",
        "        if base_indicator in FRED_METADATA:\n",
        "            full_meta = FRED_METADATA[base_indicator].copy()\n",
        "            full_meta.update({\n",
        "                'lag_days': lag_days,\n",
        "                'total_lag': total_lag,\n",
        "                'actual_frequency': data_type,\n",
        "                'report_window': report_window,\n",
        "                'column_name': col_name,\n",
        "                'indicator_with_lag': indicator_name\n",
        "            })\n",
        "        else:\n",
        "            full_meta = {\n",
        "                'name': base_indicator,\n",
        "                'lag_days': lag_days,\n",
        "                'total_lag': total_lag,\n",
        "                'frequency': data_type,\n",
        "                'actual_frequency': data_type,\n",
        "                'report_window': report_window,\n",
        "                'column_name': col_name,\n",
        "                'indicator_with_lag': indicator_name,\n",
        "                'typical_release': f'{data_type} data with {lag_days}d lag'\n",
        "            }\n",
        "\n",
        "        macro_features_added[col_name] = full_meta\n",
        "\n",
        "    # Apply forward fill to macro data\n",
        "    macro_cols = [col for col in macro_df.columns if col.startswith('fred_')]\n",
        "    macro_df[macro_cols] = macro_df[macro_cols].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    merged_stock_data = {}\n",
        "    for ticker, stock_df in stock_data.items():\n",
        "        if 'Date' not in stock_df.columns:\n",
        "            merged_stock_data[ticker] = stock_df\n",
        "            continue\n",
        "\n",
        "        stock_df = stock_df.copy()\n",
        "        stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
        "\n",
        "        merged_df = pd.merge(stock_df, macro_df, on='Date', how='left')\n",
        "\n",
        "        # Remove USREC and other binary features after merge\n",
        "        binary_features_to_remove = []\n",
        "        for col in merged_df.columns:\n",
        "            if col.startswith('fred_'):\n",
        "                base_name = col.replace('fred_', '').split('_')[0].upper()\n",
        "                if base_name in CONFIG['EXCLUDE_FROM_SHAP']:\n",
        "                    binary_features_to_remove.append(col)\n",
        "\n",
        "        if binary_features_to_remove:\n",
        "            logger.info(f\"Removing binary features from {ticker}: {binary_features_to_remove}\")\n",
        "            merged_df = merged_df.drop(columns=binary_features_to_remove)\n",
        "            for feat in binary_features_to_remove:\n",
        "                if feat in macro_features_added:\n",
        "                    del macro_features_added[feat]\n",
        "\n",
        "        merged_stock_data[ticker] = merged_df\n",
        "\n",
        "        # Data sanity check after merge\n",
        "        logger.info(f\"{ticker} - Shape after merge: {merged_df.shape}\")\n",
        "\n",
        "    logger.info(f\"Added {len(macro_features_added)} macro features (after filtering)\")\n",
        "\n",
        "    return merged_stock_data, macro_features_added\n",
        "\n",
        "# ==========================\n",
        "# FEATURE ENGINEERING\n",
        "# ==========================\n",
        "\n",
        "def create_proprietary_features(df):\n",
        "    \"\"\"Create or ensure ALL proprietary features exist\"\"\"\n",
        "    features = pd.DataFrame(index=df.index)\n",
        "\n",
        "    # Ensure we have required price data\n",
        "    if 'Close' not in df.columns:\n",
        "        logger.error(\"No Close price data available!\")\n",
        "        return features\n",
        "\n",
        "    close_prices = df['Close']\n",
        "    high_prices = df.get('High', close_prices)\n",
        "    low_prices = df.get('Low', close_prices)\n",
        "    volume = df.get('Volume', pd.Series(1000000, index=df.index))\n",
        "\n",
        "    # Calculate returns for various features\n",
        "    returns = close_prices.pct_change()\n",
        "\n",
        "    # VIX - if not present, calculate from volatility\n",
        "    if 'VIX' in df.columns:\n",
        "        features['VIX'] = df['VIX']\n",
        "    else:\n",
        "        # Estimate VIX from 30-day volatility\n",
        "        features['VIX'] = returns.rolling(30).std() * np.sqrt(252) * 100\n",
        "        features['VIX'] = features['VIX'].fillna(20)  # Default VIX\n",
        "\n",
        "    # FNG (Fear & Greed) - if not present, create synthetic\n",
        "    if 'FNG' in df.columns:\n",
        "        features['FNG'] = df['FNG']\n",
        "    else:\n",
        "        # Synthetic FNG based on momentum and volatility\n",
        "        momentum = close_prices.pct_change(20)\n",
        "        volatility = returns.rolling(20).std()\n",
        "        # Higher momentum and lower volatility = higher greed\n",
        "        features['FNG'] = 50 + (momentum * 1000) - (volatility * 500)\n",
        "        features['FNG'] = features['FNG'].clip(0, 100).fillna(50)\n",
        "\n",
        "    # RSI\n",
        "    if 'RSI' in df.columns:\n",
        "        features['RSI'] = df['RSI']\n",
        "    else:\n",
        "        features['RSI'] = calculate_rsi(close_prices)\n",
        "        features['RSI'] = features['RSI'].fillna(50)\n",
        "\n",
        "    # Annual Volatility\n",
        "    if 'AnnVolatility' in df.columns:\n",
        "        features['AnnVolatility'] = df['AnnVolatility']\n",
        "    else:\n",
        "        features['AnnVolatility'] = returns.rolling(252).std() * np.sqrt(252) * 100\n",
        "        features['AnnVolatility'] = features['AnnVolatility'].fillna(20)\n",
        "\n",
        "    # Momentum 125\n",
        "    if 'Momentum125' in df.columns:\n",
        "        features['Momentum125'] = df['Momentum125']\n",
        "    else:\n",
        "        features['Momentum125'] = calculate_momentum_125(close_prices)\n",
        "        features['Momentum125'] = features['Momentum125'].fillna(0)\n",
        "\n",
        "    # Price Strength\n",
        "    if 'PriceStrength' in df.columns:\n",
        "        features['PriceStrength'] = df['PriceStrength']\n",
        "    else:\n",
        "        features['PriceStrength'] = calculate_price_strength(close_prices, volume)\n",
        "        features['PriceStrength'] = features['PriceStrength'].fillna(0)\n",
        "\n",
        "    # Volume Breadth\n",
        "    if 'VolumeBreadth' in df.columns:\n",
        "        features['VolumeBreadth'] = df['VolumeBreadth']\n",
        "    else:\n",
        "        features['VolumeBreadth'] = calculate_volume_breadth(volume, close_prices)\n",
        "        features['VolumeBreadth'] = features['VolumeBreadth'].fillna(1)\n",
        "\n",
        "    # Call/Put Ratio - if not present, use synthetic\n",
        "    if 'CallPut' in df.columns:\n",
        "        features['CallPut'] = df['CallPut']\n",
        "    else:\n",
        "        # Synthetic based on trend and volatility\n",
        "        trend = close_prices.rolling(20).mean() / close_prices.rolling(50).mean()\n",
        "        features['CallPut'] = 50 * trend\n",
        "        features['CallPut'] = features['CallPut'].fillna(50)\n",
        "\n",
        "    # News Score - if not present, use 5 (neutral)\n",
        "    if 'NewsScore' in df.columns:\n",
        "        features['NewsScore'] = df['NewsScore']\n",
        "    else:\n",
        "        features['NewsScore'] = 5  # Neutral sentiment\n",
        "\n",
        "    # MACD\n",
        "    if 'MACD' in df.columns:\n",
        "        features['MACD'] = df['MACD']\n",
        "    else:\n",
        "        macd_data = calculate_macd(close_prices)\n",
        "        features['MACD'] = macd_data['macd']\n",
        "        features['MACD'] = features['MACD'].fillna(0)\n",
        "\n",
        "    # Bollinger Band Width\n",
        "    if 'BollingerBandWidth' in df.columns:\n",
        "        features['BollingerBandWidth'] = df['BollingerBandWidth']\n",
        "    else:\n",
        "        bb_data = calculate_bollinger_bands(close_prices)\n",
        "        features['BollingerBandWidth'] = bb_data['bandwidth'] * 100\n",
        "        features['BollingerBandWidth'] = features['BollingerBandWidth'].fillna(2)\n",
        "\n",
        "    # ATR\n",
        "    if 'ATR' in df.columns:\n",
        "        features['ATR'] = df['ATR']\n",
        "    else:\n",
        "        features['ATR'] = calculate_atr(high_prices, low_prices, close_prices)\n",
        "        features['ATR'] = features['ATR'].fillna(1)\n",
        "\n",
        "    # Stochastic RSI\n",
        "    if 'StochRSI' in df.columns:\n",
        "        features['StochRSI'] = df['StochRSI']\n",
        "    else:\n",
        "        rsi = features['RSI']\n",
        "        features['StochRSI'] = calculate_stochastic_rsi(rsi)\n",
        "        features['StochRSI'] = features['StochRSI'].fillna(50)\n",
        "\n",
        "    # Additional indicators\n",
        "    # OBV\n",
        "    if 'OBV' in df.columns:\n",
        "        features['OBV'] = df['OBV']\n",
        "    else:\n",
        "        features['OBV'] = calculate_obv(close_prices, volume)\n",
        "        features['OBV'] = features['OBV'].fillna(0)\n",
        "\n",
        "    # CMF\n",
        "    if 'CMF' in df.columns:\n",
        "        features['CMF'] = df['CMF']\n",
        "    else:\n",
        "        features['CMF'] = calculate_cmf(high_prices, low_prices, close_prices, volume)\n",
        "        features['CMF'] = features['CMF'].fillna(0)\n",
        "\n",
        "    # ADX\n",
        "    if 'ADX' in df.columns:\n",
        "        features['ADX'] = df['ADX']\n",
        "    else:\n",
        "        features['ADX'] = calculate_adx(high_prices, low_prices, close_prices)\n",
        "        features['ADX'] = features['ADX'].fillna(25)\n",
        "\n",
        "    # Williams %R\n",
        "    if 'Williams_R' in df.columns:\n",
        "        features['Williams_R'] = df['Williams_R']\n",
        "    else:\n",
        "        features['Williams_R'] = calculate_williams_r(high_prices, low_prices, close_prices)\n",
        "        features['Williams_R'] = features['Williams_R'].fillna(-50)\n",
        "\n",
        "    # CCI\n",
        "    if 'CCI' in df.columns:\n",
        "        features['CCI'] = df['CCI']\n",
        "    else:\n",
        "        features['CCI'] = calculate_cci(high_prices, low_prices, close_prices)\n",
        "        features['CCI'] = features['CCI'].fillna(0)\n",
        "\n",
        "    # MFI\n",
        "    if 'MFI' in df.columns:\n",
        "        features['MFI'] = df['MFI']\n",
        "    else:\n",
        "        features['MFI'] = calculate_mfi(high_prices, low_prices, close_prices, volume)\n",
        "        features['MFI'] = features['MFI'].fillna(50)\n",
        "\n",
        "    # Fill any remaining NaN values with appropriate defaults\n",
        "    features = features.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    # Final fill with defaults\n",
        "    for col in features.columns:\n",
        "        if features[col].isna().any():\n",
        "            if col in ['VIX', 'AnnVolatility']:\n",
        "                features[col] = features[col].fillna(20)\n",
        "            elif col in ['FNG', 'RSI', 'StochRSI', 'MFI', 'CallPut']:\n",
        "                features[col] = features[col].fillna(50)\n",
        "            elif col in ['ADX']:\n",
        "                features[col] = features[col].fillna(25)\n",
        "            elif col in ['Williams_R']:\n",
        "                features[col] = features[col].fillna(-50)\n",
        "            elif col == 'NewsScore':\n",
        "                features[col] = features[col].fillna(5)\n",
        "            elif col == 'VolumeBreadth':\n",
        "                features[col] = features[col].fillna(1)\n",
        "            elif col == 'BollingerBandWidth':\n",
        "                features[col] = features[col].fillna(2)\n",
        "            else:\n",
        "                features[col] = features[col].fillna(0)\n",
        "\n",
        "    logger.info(f\"Created {len(features.columns)} proprietary features: {list(features.columns)}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_regime_features(proprietary_features):\n",
        "    \"\"\"Create binary regime features based on thresholds\"\"\"\n",
        "    regime_features = pd.DataFrame(index=proprietary_features.index)\n",
        "\n",
        "    for feature, thresholds in CONFIG['REGIME_THRESHOLDS'].items():\n",
        "        if feature in proprietary_features.columns:\n",
        "            feat_data = proprietary_features[feature]\n",
        "\n",
        "            # Create all regime levels\n",
        "            for level in ['extreme_high', 'high', 'low', 'extreme_low']:\n",
        "                if level in thresholds:\n",
        "                    if 'high' in level:\n",
        "                        regime_features[f'{feature}_{level}'] = (\n",
        "                            feat_data > thresholds[level]\n",
        "                        ).astype(int)\n",
        "                    else:  # low levels\n",
        "                        regime_features[f'{feature}_{level}'] = (\n",
        "                            feat_data < thresholds[level]\n",
        "                        ).astype(int)\n",
        "\n",
        "            # Create combined regime indicators\n",
        "            if 'high' in thresholds and 'low' in thresholds:\n",
        "                regime_features[f'{feature}_neutral'] = (\n",
        "                    (feat_data >= thresholds['low']) &\n",
        "                    (feat_data <= thresholds['high'])\n",
        "                ).astype(int)\n",
        "\n",
        "    logger.info(f\"Created {len(regime_features.columns)} regime features\")\n",
        "\n",
        "    return regime_features\n",
        "\n",
        "def create_nonlinear_transformations(features):\n",
        "    \"\"\"Create non-linear transformations of proprietary features\"\"\"\n",
        "    transformed_features = pd.DataFrame(index=features.index)\n",
        "\n",
        "    for col in features.columns:\n",
        "        if col in CONFIG['PROPRIETARY_FEATURES']:\n",
        "            feat_data = features[col]\n",
        "\n",
        "            # Log transformation (handle negative values)\n",
        "            min_val = feat_data.min()\n",
        "            if min_val <= 0:\n",
        "                # Shift to make all values positive\n",
        "                shifted_data = feat_data - min_val + 1\n",
        "                transformed_features[f'{col}_log'] = np.log(shifted_data)\n",
        "            else:\n",
        "                transformed_features[f'{col}_log'] = np.log(feat_data + 1e-8)\n",
        "\n",
        "            # Square transformation (for capturing extremes)\n",
        "            transformed_features[f'{col}_square'] = feat_data ** 2\n",
        "\n",
        "            # Square root (for positive values)\n",
        "            if min_val >= 0:\n",
        "                transformed_features[f'{col}_sqrt'] = np.sqrt(feat_data + 1e-8)\n",
        "            else:\n",
        "                # Use absolute value\n",
        "                transformed_features[f'{col}_sqrt'] = np.sqrt(np.abs(feat_data) + 1e-8)\n",
        "\n",
        "            # Rank transformation\n",
        "            transformed_features[f'{col}_rank'] = rankdata(feat_data) / len(feat_data)\n",
        "\n",
        "            # Percentile transformation\n",
        "            transformed_features[f'{col}_pct'] = feat_data.rank(pct=True)\n",
        "\n",
        "            # Z-score normalization\n",
        "            mean_val = feat_data.mean()\n",
        "            std_val = feat_data.std()\n",
        "            if std_val > 0:\n",
        "                transformed_features[f'{col}_zscore'] = (feat_data - mean_val) / std_val\n",
        "            else:\n",
        "                transformed_features[f'{col}_zscore'] = 0\n",
        "\n",
        "    # Clip extreme values\n",
        "    for col in transformed_features.columns:\n",
        "        transformed_features[col] = transformed_features[col].clip(-10, 10)\n",
        "\n",
        "    logger.info(f\"Created {len(transformed_features.columns)} non-linear transformations\")\n",
        "\n",
        "    return transformed_features\n",
        "\n",
        "def create_comprehensive_interaction_features(macro_features, proprietary_features, regime_features):\n",
        "    \"\"\"Create extensive interaction features between all feature types\"\"\"\n",
        "    interaction_features = pd.DataFrame(index=macro_features.index)\n",
        "\n",
        "    # Get feature columns by type\n",
        "    macro_cols = [col for col in macro_features.columns if 'fred_' in col]\n",
        "    prop_cols = [col for col in proprietary_features.columns if col in CONFIG['PROPRIETARY_FEATURES']]\n",
        "    regime_cols = regime_features.columns.tolist()\n",
        "\n",
        "    logger.info(f\"Creating comprehensive interactions: {len(macro_cols)} macro × {len(prop_cols)} proprietary\")\n",
        "\n",
        "    # 1. Macro × Proprietary interactions (ALL combinations)\n",
        "    interaction_count = 0\n",
        "    for macro_col in macro_cols:\n",
        "        for prop_col in prop_cols:\n",
        "            interaction_name = f\"{macro_col}_X_{prop_col}\"\n",
        "            interaction_features[interaction_name] = macro_features[macro_col] * proprietary_features[prop_col]\n",
        "            interaction_count += 1\n",
        "\n",
        "    logger.info(f\"Created {interaction_count} macro × proprietary interactions\")\n",
        "\n",
        "    # 2. Macro × Regime interactions\n",
        "    regime_count = 0\n",
        "    for macro_col in macro_cols[:20]:  # Top 20 macro features\n",
        "        for regime_col in regime_cols:\n",
        "            if any(prop in regime_col for prop in ['VIX', 'FNG', 'RSI', 'Momentum125']):  # Focus on key regimes\n",
        "                interaction_name = f\"{macro_col}_X_{regime_col}\"\n",
        "                interaction_features[interaction_name] = macro_features[macro_col] * regime_features[regime_col]\n",
        "                regime_count += 1\n",
        "\n",
        "    logger.info(f\"Created {regime_count} macro × regime interactions\")\n",
        "\n",
        "    # 3. High-importance triple interactions\n",
        "    important_interactions = [\n",
        "        ('CPIAUCSL', 'VIX', 'VIX_high'),\n",
        "        ('DGS10', 'AnnVolatility', 'VIX_extreme_high'),\n",
        "        ('UNRATE', 'FNG', 'FNG_low'),\n",
        "        ('GDP', 'Momentum125', 'Momentum125_high'),\n",
        "        ('FEDFUNDS', 'PriceStrength', 'RSI_extreme_high'),\n",
        "        ('AMERIBOR', 'VolumeBreadth', 'VIX_low'),\n",
        "        ('PAYEMS', 'RSI', 'FNG_extreme_low'),\n",
        "        ('RETAILSL', 'MACD', 'Momentum125_low')\n",
        "    ]\n",
        "\n",
        "    triple_count = 0\n",
        "    for macro_base, prop, regime in important_interactions:\n",
        "        # Find matching columns\n",
        "        macro_col = None\n",
        "        for col in macro_cols:\n",
        "            if macro_base in col:\n",
        "                macro_col = col\n",
        "                break\n",
        "\n",
        "        if macro_col and prop in proprietary_features.columns and regime in regime_features.columns:\n",
        "            interaction_name = f\"{macro_col}_X_{prop}_X_{regime}\"\n",
        "            interaction_features[interaction_name] = (\n",
        "                macro_features[macro_col] *\n",
        "                proprietary_features[prop] *\n",
        "                regime_features[regime]\n",
        "            )\n",
        "            triple_count += 1\n",
        "\n",
        "    logger.info(f\"Created {triple_count} triple interactions\")\n",
        "\n",
        "    # 4. Proprietary × Proprietary interactions (key pairs)\n",
        "    prop_interactions = [\n",
        "        ('VIX', 'FNG'),\n",
        "        ('VIX', 'RSI'),\n",
        "        ('VIX', 'Momentum125'),\n",
        "        ('FNG', 'RSI'),\n",
        "        ('Momentum125', 'PriceStrength'),\n",
        "        ('AnnVolatility', 'VolumeBreadth'),\n",
        "        ('MACD', 'RSI'),\n",
        "        ('ATR', 'ADX')\n",
        "    ]\n",
        "\n",
        "    prop_count = 0\n",
        "    for prop1, prop2 in prop_interactions:\n",
        "        if prop1 in proprietary_features.columns and prop2 in proprietary_features.columns:\n",
        "            interaction_name = f\"{prop1}_X_{prop2}\"\n",
        "            interaction_features[interaction_name] = (\n",
        "                proprietary_features[prop1] * proprietary_features[prop2]\n",
        "            )\n",
        "            prop_count += 1\n",
        "\n",
        "    logger.info(f\"Created {prop_count} proprietary × proprietary interactions\")\n",
        "\n",
        "    # Normalize interaction features\n",
        "    for col in interaction_features.columns:\n",
        "        # Clip extreme values\n",
        "        interaction_features[col] = interaction_features[col].clip(-1000, 1000)\n",
        "\n",
        "        # Standardize\n",
        "        mean_val = interaction_features[col].mean()\n",
        "        std_val = interaction_features[col].std()\n",
        "        if std_val > 0:\n",
        "            interaction_features[col] = (interaction_features[col] - mean_val) / std_val\n",
        "            interaction_features[col] = interaction_features[col].clip(-5, 5)\n",
        "\n",
        "    total_interactions = len(interaction_features.columns)\n",
        "    logger.info(f\"Created {total_interactions} total interaction features\")\n",
        "\n",
        "    return interaction_features\n",
        "\n",
        "def create_technical_features(df):\n",
        "    \"\"\"Create comprehensive technical features\"\"\"\n",
        "    features = pd.DataFrame(index=df.index)\n",
        "\n",
        "    # Price-based features\n",
        "    features['returns_1d'] = df['Close'].pct_change()\n",
        "    features['returns_5d'] = df['Close'].pct_change(5)\n",
        "    features['returns_20d'] = df['Close'].pct_change(20)\n",
        "    features['returns_60d'] = df['Close'].pct_change(60)\n",
        "\n",
        "    # Log returns\n",
        "    close_shifted = df['Close'].shift(1)\n",
        "    mask = (df['Close'] > 0) & (close_shifted > 0)\n",
        "    features['log_returns_1d'] = 0.0\n",
        "    features.loc[mask, 'log_returns_1d'] = np.log(df.loc[mask, 'Close'] / close_shifted[mask])\n",
        "\n",
        "    # Moving averages\n",
        "    for period in [5, 10, 20, 50, 100, 200]:\n",
        "        if len(df) >= period:\n",
        "            sma = df['Close'].rolling(window=period).mean()\n",
        "            ema = df['Close'].ewm(span=period).mean()\n",
        "\n",
        "            features[f'price_to_sma_{period}'] = 1.0\n",
        "            features[f'price_to_ema_{period}'] = 1.0\n",
        "\n",
        "            mask_sma = sma > 0\n",
        "            mask_ema = ema > 0\n",
        "\n",
        "            features.loc[mask_sma, f'price_to_sma_{period}'] = df.loc[mask_sma, 'Close'] / sma[mask_sma]\n",
        "            features.loc[mask_ema, f'price_to_ema_{period}'] = df.loc[mask_ema, 'Close'] / ema[mask_ema]\n",
        "\n",
        "            features[f'sma_{period}_slope'] = sma.pct_change(5)\n",
        "\n",
        "    # Support/Resistance\n",
        "    high_20d = df['High'].rolling(20).max()\n",
        "    low_20d = df['Low'].rolling(20).min()\n",
        "    high_52w = df['High'].rolling(252).max()\n",
        "    low_52w = df['Low'].rolling(252).min()\n",
        "\n",
        "    features['dist_from_high_20d'] = 1.0\n",
        "    features['dist_from_low_20d'] = 1.0\n",
        "    features['dist_from_high_52w'] = 1.0\n",
        "    features['dist_from_low_52w'] = 1.0\n",
        "\n",
        "    mask_h20 = high_20d > 0\n",
        "    mask_l20 = low_20d > 0\n",
        "    mask_h52 = high_52w > 0\n",
        "    mask_l52 = low_52w > 0\n",
        "\n",
        "    features.loc[mask_h20, 'dist_from_high_20d'] = df.loc[mask_h20, 'Close'] / high_20d[mask_h20]\n",
        "    features.loc[mask_l20, 'dist_from_low_20d'] = df.loc[mask_l20, 'Close'] / low_20d[mask_l20]\n",
        "    features.loc[mask_h52, 'dist_from_high_52w'] = df.loc[mask_h52, 'Close'] / high_52w[mask_h52]\n",
        "    features.loc[mask_l52, 'dist_from_low_52w'] = df.loc[mask_l52, 'Close'] / low_52w[mask_l52]\n",
        "\n",
        "    # Trend strength\n",
        "    features['trend_strength_10d'] = calculate_trend_strength(df['Close'], 10)\n",
        "    features['trend_strength_20d'] = calculate_trend_strength(df['Close'], 20)\n",
        "    features['trend_strength_60d'] = calculate_trend_strength(df['Close'], 60)\n",
        "\n",
        "    # Pattern recognition\n",
        "    features['higher_highs'] = (df['High'] > df['High'].shift(1)).astype(int).rolling(20).sum()\n",
        "    features['lower_lows'] = (df['Low'] < df['Low'].shift(1)).astype(int).rolling(20).sum()\n",
        "\n",
        "    # Final safety check\n",
        "    for col in features.columns:\n",
        "        if features[col].dtype in ['float64', 'float32']:\n",
        "            features[col] = features[col].clip(lower=-1000, upper=1000)\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_macro_features(df, macro_metadata, debug=False):\n",
        "    \"\"\"Create macro features - preserving all features\"\"\"\n",
        "    features = pd.DataFrame(index=df.index)\n",
        "\n",
        "    macro_cols = [c for c in df.columns if c.startswith('fred_')]\n",
        "\n",
        "    if debug:\n",
        "        logger.info(f\"Creating macro features from {len(macro_cols)} columns...\")\n",
        "\n",
        "    if not macro_cols:\n",
        "        if debug:\n",
        "            logger.warning(\"No macro columns (fred_*) found in DataFrame!\")\n",
        "        return features\n",
        "\n",
        "    # Process ALL macro columns without filtering\n",
        "    for col in macro_cols:\n",
        "        # Skip if too many NaN\n",
        "        if df[col].notna().sum() < len(df) * 0.1:\n",
        "            continue\n",
        "\n",
        "        # Get the base data\n",
        "        col_data = df[col].copy()\n",
        "\n",
        "        # Fill forward then backward to handle NaN\n",
        "        col_data = col_data.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "        # Keep the raw feature\n",
        "        features[col] = col_data\n",
        "\n",
        "        # Normalized version (z-score)\n",
        "        rolling_mean = col_data.rolling(252, min_periods=60).mean()\n",
        "        rolling_std = col_data.rolling(252, min_periods=60).std()\n",
        "\n",
        "        zscore_col = f'{col}_zscore'\n",
        "        features[zscore_col] = 0.0\n",
        "\n",
        "        valid_mask = (rolling_std > 1e-6) & rolling_std.notna() & rolling_mean.notna()\n",
        "        if valid_mask.any():\n",
        "            features.loc[valid_mask, zscore_col] = (\n",
        "                (col_data[valid_mask] - rolling_mean[valid_mask]) / rolling_std[valid_mask]\n",
        "            )\n",
        "\n",
        "        features[zscore_col] = features[zscore_col].clip(-5, 5)\n",
        "\n",
        "        # Rate of change\n",
        "        features[f'{col}_roc_5d'] = col_data.pct_change(5).clip(-2, 2)\n",
        "        features[f'{col}_roc_20d'] = col_data.pct_change(20).clip(-2, 2)\n",
        "\n",
        "        # Trend\n",
        "        trend_col = f'{col}_trend'\n",
        "        trend_values = calculate_trend_strength(col_data, 60)\n",
        "        features[trend_col] = trend_values.clip(-5, 5)\n",
        "\n",
        "        # Momentum\n",
        "        momentum_col = f'{col}_momentum'\n",
        "        momentum_values = col_data - col_data.shift(20)\n",
        "\n",
        "        col_scale = col_data.abs().rolling(60, min_periods=20).mean()\n",
        "        features[momentum_col] = 0.0\n",
        "        scale_mask = col_scale > 1e-6\n",
        "        if scale_mask.any():\n",
        "            features.loc[scale_mask, momentum_col] = (\n",
        "                momentum_values[scale_mask] / col_scale[scale_mask]\n",
        "            ).clip(-5, 5)\n",
        "\n",
        "    # Final safety check\n",
        "    for col in features.columns:\n",
        "        features[col] = features[col].replace([np.inf, -np.inf], 0)\n",
        "        features[col] = features[col].fillna(0)\n",
        "        features[col] = features[col].clip(-10, 10)\n",
        "\n",
        "    if debug:\n",
        "        logger.info(f\"Created {len(features.columns)} macro features\")\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_all_features(df, macro_metadata=None):\n",
        "    \"\"\"Create all features: technical, proprietary, macro, regime, transformations, and interactions\"\"\"\n",
        "\n",
        "    logger.info(\"Creating comprehensive feature set with ALL proprietary features...\")\n",
        "\n",
        "    # 1. Technical features\n",
        "    tech_features = create_technical_features(df)\n",
        "    logger.info(f\"Created {len(tech_features.columns)} technical features\")\n",
        "\n",
        "    # 2. Proprietary features - FORCE CREATION OF ALL\n",
        "    proprietary_features = create_proprietary_features(df)\n",
        "    logger.info(f\"Created/verified {len(proprietary_features.columns)} proprietary features\")\n",
        "\n",
        "    # 3. Macro features\n",
        "    macro_features = create_macro_features(df, macro_metadata)\n",
        "    logger.info(f\"Created {len(macro_features.columns)} macro features\")\n",
        "\n",
        "    # 4. Regime features\n",
        "    regime_features = create_regime_features(proprietary_features)\n",
        "    logger.info(f\"Created {len(regime_features.columns)} regime features\")\n",
        "\n",
        "    # 5. Non-linear transformations\n",
        "    transformed_features = create_nonlinear_transformations(proprietary_features)\n",
        "    logger.info(f\"Created {len(transformed_features.columns)} non-linear transformations\")\n",
        "\n",
        "    # 6. Comprehensive interaction features\n",
        "    interaction_features = create_comprehensive_interaction_features(\n",
        "        macro_features, proprietary_features, regime_features\n",
        "    )\n",
        "    logger.info(f\"Created {len(interaction_features.columns)} interaction features\")\n",
        "\n",
        "    # Combine all features\n",
        "    all_features = pd.concat([\n",
        "        tech_features,\n",
        "        proprietary_features,\n",
        "        macro_features,\n",
        "        regime_features,\n",
        "        transformed_features,\n",
        "        interaction_features\n",
        "    ], axis=1)\n",
        "\n",
        "    # DO NOT remove any features based on variance - keep ALL proprietary features\n",
        "\n",
        "    # Clean the features\n",
        "    all_features = all_features.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Cap extreme values\n",
        "    for col in all_features.columns:\n",
        "        if all_features[col].dtype in ['float64', 'float32']:\n",
        "            valid_values = all_features[col].dropna()\n",
        "            if len(valid_values) > 0:\n",
        "                q001 = valid_values.quantile(0.001)\n",
        "                q999 = valid_values.quantile(0.999)\n",
        "\n",
        "                if np.isfinite(q001) and np.isfinite(q999):\n",
        "                    all_features[col] = all_features[col].clip(lower=q001, upper=q999)\n",
        "\n",
        "    # Enhanced missing data handling\n",
        "    all_features = all_features.fillna(method='ffill', limit=5)\n",
        "    all_features = all_features.fillna(method='bfill', limit=5)\n",
        "    all_features = all_features.fillna(0)\n",
        "\n",
        "    # Log feature type distribution\n",
        "    feature_types = {\n",
        "        'Technical': len([c for c in all_features.columns if c in tech_features.columns]),\n",
        "        'Proprietary': len([c for c in all_features.columns if c in proprietary_features.columns]),\n",
        "        'Macro': len([c for c in all_features.columns if c in macro_features.columns]),\n",
        "        'Regime': len([c for c in all_features.columns if c in regime_features.columns]),\n",
        "        'Transformed': len([c for c in all_features.columns if c in transformed_features.columns]),\n",
        "        'Interaction': len([c for c in all_features.columns if c in interaction_features.columns])\n",
        "    }\n",
        "\n",
        "    logger.info(f\"\\nFeature distribution:\")\n",
        "    for feat_type, count in feature_types.items():\n",
        "        logger.info(f\"  {feat_type}: {count} features\")\n",
        "    logger.info(f\"  TOTAL: {len(all_features.columns)} features\")\n",
        "\n",
        "    # Verify all proprietary features are present\n",
        "    missing_proprietary = []\n",
        "    for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "        if feat not in all_features.columns:\n",
        "            missing_proprietary.append(feat)\n",
        "\n",
        "    if missing_proprietary:\n",
        "        logger.warning(f\"Missing proprietary features in final set: {missing_proprietary}\")\n",
        "    else:\n",
        "        logger.info(\"✓ All proprietary features successfully included!\")\n",
        "\n",
        "    return all_features\n",
        "\n",
        "# ==========================\n",
        "# ENHANCED ML MODEL WITH SHAP\n",
        "# ==========================\n",
        "\n",
        "class EnhancedTradingModel:\n",
        "    \"\"\"Enhanced trading model with comprehensive SHAP analysis and multi-horizon support\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.feature_columns = {}\n",
        "        self.feature_metadata = {}\n",
        "        self.shap_explainers = {}\n",
        "        self.training_diagnostics = {}\n",
        "        self.performance_metrics = {}\n",
        "        self.signal_statistics = defaultdict(lambda: defaultdict(int))\n",
        "        self.per_stock_metrics = {}\n",
        "        self.feature_importance_matrix = {}\n",
        "        self.feature_presence_matrix = {}  # Track which features appear in top 5\n",
        "\n",
        "    def prepare_training_data(self, df, features, prediction_days):\n",
        "        \"\"\"Prepare training data with proper future alignment\"\"\"\n",
        "        # Calculate future returns\n",
        "        future_price = df['Close'].shift(-prediction_days)\n",
        "        current_price = df['Close']\n",
        "        future_returns = (future_price - current_price) / current_price\n",
        "\n",
        "        # Binary target: 1 for up, 0 for down\n",
        "        target = (future_returns > 0).astype(int)\n",
        "\n",
        "        # Store actual returns for performance calculation\n",
        "        actual_returns = future_returns\n",
        "\n",
        "        # Remove last rows without future data\n",
        "        features = features.iloc[:-prediction_days]\n",
        "        target = target.iloc[:-prediction_days]\n",
        "        actual_returns = actual_returns.iloc[:-prediction_days]\n",
        "\n",
        "        # Remove any NaN\n",
        "        valid_idx = ~(features.isna().any(axis=1) | target.isna())\n",
        "\n",
        "        X = features[valid_idx].copy()\n",
        "        y = target[valid_idx].copy()\n",
        "        returns = actual_returns[valid_idx].copy()\n",
        "\n",
        "        # Check class balance\n",
        "        if len(y) > 0:\n",
        "            up_count = (y == 1).sum()\n",
        "            down_count = (y == 0).sum()\n",
        "            logger.info(f\"Target balance for {prediction_days}d: UP={up_count} ({up_count/len(y)*100:.1f}%), DOWN={down_count} ({down_count/len(y)*100:.1f}%)\")\n",
        "\n",
        "        return X, y, returns\n",
        "\n",
        "    def categorize_features(self, feature_names):\n",
        "        \"\"\"Categorize features by type\"\"\"\n",
        "        categories = {\n",
        "            'macro': [],\n",
        "            'proprietary': [],\n",
        "            'technical': [],\n",
        "            'regime': [],\n",
        "            'transformed': [],\n",
        "            'interaction': []\n",
        "        }\n",
        "\n",
        "        for feat in feature_names:\n",
        "            if '_X_' in feat:\n",
        "                categories['interaction'].append(feat)\n",
        "            elif feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "                categories['proprietary'].append(feat)\n",
        "            elif any(f'{prop}_' in feat for prop in CONFIG['PROPRIETARY_FEATURES']):\n",
        "                if any(transform in feat for transform in ['_log', '_square', '_sqrt', '_rank', '_pct', '_zscore']):\n",
        "                    categories['transformed'].append(feat)\n",
        "                elif any(regime in feat for regime in ['_high', '_low', '_extreme', '_neutral']):\n",
        "                    categories['regime'].append(feat)\n",
        "                else:\n",
        "                    categories['technical'].append(feat)\n",
        "            elif 'fred_' in feat:\n",
        "                categories['macro'].append(feat)\n",
        "            else:\n",
        "                categories['technical'].append(feat)\n",
        "\n",
        "        return categories\n",
        "\n",
        "    def get_top_features_by_type(self, feature_names, shap_values, n_per_type=5):\n",
        "        \"\"\"Get top N features from each category\"\"\"\n",
        "        # Calculate absolute SHAP importance\n",
        "        shap_importance = np.abs(shap_values).mean(axis=0) if len(shap_values.shape) > 1 else np.abs(shap_values)\n",
        "\n",
        "        # Categorize features\n",
        "        categories = self.categorize_features(feature_names)\n",
        "\n",
        "        # Get top features by category\n",
        "        top_features = {}\n",
        "\n",
        "        for category, feat_list in categories.items():\n",
        "            category_features = []\n",
        "            for i, feat_name in enumerate(feature_names):\n",
        "                if feat_name in feat_list:\n",
        "                    category_features.append((feat_name, shap_importance[i], i))\n",
        "\n",
        "            # Sort by importance\n",
        "            category_features.sort(key=lambda x: x[1], reverse=True)\n",
        "            top_features[category] = category_features[:n_per_type]\n",
        "\n",
        "        # Also get overall top 5\n",
        "        all_features = [(feat_name, shap_importance[i], i) for i, feat_name in enumerate(feature_names)]\n",
        "        all_features.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_features['overall_top_5'] = all_features[:5]\n",
        "\n",
        "        return top_features\n",
        "\n",
        "    def calculate_per_stock_metrics(self, stock_data, prediction_days):\n",
        "        \"\"\"Calculate test metrics and feature importance for each individual stock\"\"\"\n",
        "        if prediction_days not in self.models:\n",
        "            return {}\n",
        "\n",
        "        model = self.models[prediction_days]\n",
        "        scaler = self.scalers[prediction_days]\n",
        "        feature_cols = self.feature_columns[prediction_days]\n",
        "\n",
        "        per_stock_metrics = {}\n",
        "        per_stock_feature_importance = {}\n",
        "        feature_presence = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "        for ticker, df in stock_data.items():\n",
        "            try:\n",
        "                # Create features\n",
        "                features = create_all_features(df, self.feature_metadata.get(prediction_days, {}))\n",
        "\n",
        "                # Prepare data\n",
        "                X, y, returns = self.prepare_training_data(df, features, prediction_days)\n",
        "\n",
        "                if len(X) < 50:  # Need sufficient test data\n",
        "                    continue\n",
        "\n",
        "                # Align features\n",
        "                X_aligned = pd.DataFrame(index=X.index, columns=feature_cols)\n",
        "                for col in feature_cols:\n",
        "                    if col in X.columns:\n",
        "                        X_aligned[col] = X[col]\n",
        "                    else:\n",
        "                        X_aligned[col] = 0\n",
        "\n",
        "                X_aligned = X_aligned.fillna(0)\n",
        "\n",
        "                # Split data for this stock (use last 20% as test)\n",
        "                test_size = int(len(X_aligned) * 0.2)\n",
        "                if test_size < 10:\n",
        "                    continue\n",
        "\n",
        "                X_test = X_aligned.iloc[-test_size:]\n",
        "                y_test = y.iloc[-test_size:]\n",
        "                returns_test = returns.iloc[-test_size:]\n",
        "\n",
        "                # Calculate CAGR\n",
        "                test_start_idx = len(df) - test_size - prediction_days\n",
        "                test_end_idx = len(df) - prediction_days\n",
        "\n",
        "                if test_start_idx >= 0 and test_end_idx < len(df):\n",
        "                    starting_price = df['Close'].iloc[test_start_idx]\n",
        "                    ending_price = df['Close'].iloc[test_end_idx]\n",
        "\n",
        "                    if starting_price > 0 and ending_price > 0:\n",
        "                        num_days = test_size\n",
        "                        if num_days > 0:\n",
        "                            total_return = ending_price / starting_price\n",
        "                            years = num_days / 252.0\n",
        "                            if years > 0:\n",
        "                                cagr = (total_return ** (1 / years) - 1) * 100\n",
        "                                cagr = max(-100, min(200, cagr))\n",
        "                            else:\n",
        "                                cagr = 0\n",
        "                        else:\n",
        "                            cagr = 0\n",
        "                    else:\n",
        "                        cagr = 0\n",
        "                else:\n",
        "                    cagr = 0\n",
        "\n",
        "                # Scale and predict\n",
        "                X_test_scaled = scaler.transform(X_test.values)\n",
        "                y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "                # Calculate metrics\n",
        "                metrics = calculate_performance_metrics(y_test, y_pred, returns_test, prediction_days)\n",
        "                metrics['cagr'] = cagr\n",
        "                metrics['test_samples'] = len(y_test)\n",
        "\n",
        "                per_stock_metrics[ticker] = metrics\n",
        "\n",
        "                # Calculate SHAP values for this stock's test data\n",
        "                if prediction_days in self.shap_explainers:\n",
        "                    try:\n",
        "                        # Get SHAP values for a sample of test data\n",
        "                        sample_size = min(20, len(X_test_scaled))\n",
        "                        shap_values = self.shap_explainers[prediction_days].shap_values(X_test_scaled[:sample_size])\n",
        "\n",
        "                        # Handle binary classification\n",
        "                        if isinstance(shap_values, list) and len(shap_values) == 2:\n",
        "                            shap_values = shap_values[1]\n",
        "                        elif len(shap_values.shape) == 3:\n",
        "                            shap_values = shap_values[:, :, 1]\n",
        "\n",
        "                        # Get feature importance by type\n",
        "                        feature_importance = self.get_top_features_by_type(\n",
        "                            feature_cols, shap_values, n_per_type=5\n",
        "                        )\n",
        "                        per_stock_feature_importance[ticker] = feature_importance\n",
        "\n",
        "                        # Track feature presence in top 5\n",
        "                        for feat_name, _, _ in feature_importance['overall_top_5']:\n",
        "                            categories = self.categorize_features([feat_name])\n",
        "                            for cat, feats in categories.items():\n",
        "                                if feats:\n",
        "                                    feature_presence[ticker][cat] += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.debug(f\"Could not calculate SHAP for {ticker}: {e}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"Could not calculate metrics for {ticker}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Store feature importance and presence matrices\n",
        "        if per_stock_feature_importance:\n",
        "            self.feature_importance_matrix[prediction_days] = per_stock_feature_importance\n",
        "            self.feature_presence_matrix[prediction_days] = dict(feature_presence)\n",
        "\n",
        "        return per_stock_metrics\n",
        "\n",
        "    def train_model(self, stock_data, macro_metadata, prediction_days=30):\n",
        "        \"\"\"Train model with enhanced feature diversity\"\"\"\n",
        "        logger.info(f\"\\nTraining Enhanced ML Model for {prediction_days}-day predictions...\")\n",
        "\n",
        "        all_X = []\n",
        "        all_y = []\n",
        "        all_returns = []\n",
        "        all_tickers = []\n",
        "\n",
        "        # Store macro metadata\n",
        "        self.feature_metadata[prediction_days] = macro_metadata\n",
        "\n",
        "        # Process each stock\n",
        "        for ticker, df in stock_data.items():\n",
        "            if df.empty or len(df) < CONFIG['MIN_SAMPLES_PER_TICKER']:\n",
        "                continue\n",
        "\n",
        "            # Create features - this now includes ALL proprietary features\n",
        "            features = create_all_features(df, macro_metadata)\n",
        "\n",
        "            # Prepare training data\n",
        "            X, y, returns = self.prepare_training_data(df, features, prediction_days)\n",
        "\n",
        "            if len(X) >= CONFIG['MIN_SAMPLES_FOR_TRAINING']:\n",
        "                all_X.append(X)\n",
        "                all_y.append(y)\n",
        "                all_returns.append(returns)\n",
        "                all_tickers.extend([ticker] * len(X))\n",
        "\n",
        "        if not all_X:\n",
        "            logger.error(\"Insufficient data for training\")\n",
        "            return None\n",
        "\n",
        "        # Combine all data\n",
        "        X_combined = pd.concat(all_X, ignore_index=True)\n",
        "        y_combined = pd.concat(all_y, ignore_index=True)\n",
        "        returns_combined = pd.concat(all_returns, ignore_index=True)\n",
        "\n",
        "        logger.info(f\"Combined data shape: {X_combined.shape}\")\n",
        "\n",
        "        # DIAGNOSTIC: Verify proprietary features are present\n",
        "        logger.info(f\"\\n=== VERIFYING PROPRIETARY FEATURES in training data ===\")\n",
        "        prop_features_present = []\n",
        "        prop_features_missing = []\n",
        "\n",
        "        for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "            if feat in X_combined.columns:\n",
        "                prop_features_present.append(feat)\n",
        "                non_zero = (X_combined[feat] != 0).sum()\n",
        "                unique_vals = X_combined[feat].nunique()\n",
        "                mean_val = X_combined[feat].mean()\n",
        "                logger.info(f\"  ✓ {feat}: non-zero={non_zero}, unique={unique_vals}, mean={mean_val:.2f}\")\n",
        "            else:\n",
        "                prop_features_missing.append(feat)\n",
        "                logger.warning(f\"  ✗ {feat}: MISSING from features!\")\n",
        "\n",
        "        logger.info(f\"\\nProprietary features present: {len(prop_features_present)}/{len(CONFIG['PROPRIETARY_FEATURES'])}\")\n",
        "\n",
        "        # Categorize all features\n",
        "        feature_categories = self.categorize_features(X_combined.columns.tolist())\n",
        "\n",
        "        logger.info(f\"\\n[Feature Distribution in Training Data]\")\n",
        "        for category, features in feature_categories.items():\n",
        "            logger.info(f\"  {category}: {len(features)} features\")\n",
        "        logger.info(f\"  TOTAL: {len(X_combined.columns)} features\")\n",
        "\n",
        "        # Remove truly constant features (but keep low-variance proprietary features)\n",
        "        constant_cols = []\n",
        "        for col in X_combined.columns:\n",
        "            if col not in CONFIG['PROPRIETARY_FEATURES'] and X_combined[col].nunique() <= 1:\n",
        "                constant_cols.append(col)\n",
        "\n",
        "        if constant_cols:\n",
        "            X_combined = X_combined.drop(columns=constant_cols)\n",
        "            logger.info(f\"Removed {len(constant_cols)} constant features (excluding proprietary)\")\n",
        "\n",
        "        self.feature_columns[prediction_days] = X_combined.columns.tolist()\n",
        "\n",
        "        # Unified scaling for all features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_combined)\n",
        "        self.scalers[prediction_days] = scaler\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test, returns_train, returns_test = train_test_split(\n",
        "            X_scaled, y_combined, returns_combined,\n",
        "            test_size=CONFIG['TEST_SIZE'],\n",
        "            random_state=CONFIG['RANDOM_STATE'],\n",
        "            stratify=y_combined\n",
        "        )\n",
        "\n",
        "        # Train Random Forest with updated parameters for better feature exploration\n",
        "        rf_model = RandomForestClassifier(\n",
        "            n_estimators=CONFIG['N_ESTIMATORS'],\n",
        "            max_depth=CONFIG['MAX_DEPTH'],\n",
        "            min_samples_split=CONFIG['MIN_SAMPLES_SPLIT'],\n",
        "            min_samples_leaf=CONFIG['MIN_SAMPLES_LEAF'],\n",
        "            max_features=CONFIG['MAX_FEATURES'],  # Use 50% of features at each split\n",
        "            random_state=CONFIG['RANDOM_STATE'],\n",
        "            n_jobs=-1,\n",
        "            class_weight='balanced',\n",
        "            max_samples=0.8\n",
        "        )\n",
        "\n",
        "        logger.info(\"Training Random Forest with enhanced parameters...\")\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # Get predictions\n",
        "        train_pred = rf_model.predict(X_train)\n",
        "        test_pred = rf_model.predict(X_test)\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        train_metrics = calculate_performance_metrics(y_train, train_pred, returns_train, prediction_days)\n",
        "        test_metrics = calculate_performance_metrics(y_test, test_pred, returns_test, prediction_days)\n",
        "\n",
        "        # Store metrics\n",
        "        self.performance_metrics[prediction_days] = {\n",
        "            'train': train_metrics,\n",
        "            'test': test_metrics\n",
        "        }\n",
        "\n",
        "        # Print results\n",
        "        logger.info(f\"\\nTraining Performance ({prediction_days}d):\")\n",
        "        for metric, value in train_metrics.items():\n",
        "            logger.info(f\"  {metric}: {value:.2f}\")\n",
        "\n",
        "        logger.info(f\"\\nTest Performance ({prediction_days}d):\")\n",
        "        for metric, value in test_metrics.items():\n",
        "            logger.info(f\"  {metric}: {value:.2f}\")\n",
        "\n",
        "        # Initialize SHAP explainer\n",
        "        logger.info(\"\\nInitializing SHAP explainer...\")\n",
        "        try:\n",
        "            self.shap_explainers[prediction_days] = shap.TreeExplainer(rf_model)\n",
        "\n",
        "            # Calculate sample SHAP values\n",
        "            sample_size = min(100, len(X_test))\n",
        "            X_test_sample = X_test[:sample_size]\n",
        "\n",
        "            # Get SHAP values\n",
        "            try:\n",
        "                shap_values = self.shap_explainers[prediction_days].shap_values(X_test_sample)\n",
        "            except:\n",
        "                explanation = self.shap_explainers[prediction_days](X_test_sample)\n",
        "                shap_values = explanation.values\n",
        "\n",
        "            # For binary classification, use positive class\n",
        "            if isinstance(shap_values, list) and len(shap_values) == 2:\n",
        "                shap_values = shap_values[1]\n",
        "            elif len(shap_values.shape) == 3:\n",
        "                shap_values = shap_values[:, :, 1]\n",
        "\n",
        "            # Get feature importance by type\n",
        "            feature_importance_by_type = self.get_top_features_by_type(\n",
        "                self.feature_columns[prediction_days],\n",
        "                shap_values,\n",
        "                n_per_type=10\n",
        "            )\n",
        "\n",
        "            # DIAGNOSTIC: Print top features by type\n",
        "            logger.info(f\"\\n=== TOP FEATURES BY TYPE for {prediction_days}d ===\")\n",
        "\n",
        "            for category in ['macro', 'proprietary', 'technical', 'interaction', 'regime', 'transformed']:\n",
        "                logger.info(f\"\\nTop {category.upper()} Features:\")\n",
        "                if category in feature_importance_by_type:\n",
        "                    for i, (feat_name, importance, idx) in enumerate(feature_importance_by_type[category][:5], 1):\n",
        "                        logger.info(f\"  {i}. {feat_name}: {importance:.4f}\")\n",
        "                    if not feature_importance_by_type[category]:\n",
        "                        logger.warning(f\"  ⚠️  NO {category} features in top rankings!\")\n",
        "                else:\n",
        "                    logger.warning(f\"  ⚠️  NO {category} features found!\")\n",
        "\n",
        "            # Check overall top 5\n",
        "            logger.info(f\"\\nOVERALL TOP 5 FEATURES:\")\n",
        "            overall_categories = defaultdict(int)\n",
        "            for feat_name, importance, idx in feature_importance_by_type['overall_top_5']:\n",
        "                cats = self.categorize_features([feat_name])\n",
        "                cat_name = next(iter([k for k, v in cats.items() if v]), 'unknown')\n",
        "                overall_categories[cat_name] += 1\n",
        "                logger.info(f\"  [{cat_name.upper()}] {feat_name}: {importance:.4f}\")\n",
        "\n",
        "            # Warning if proprietary features are missing from top 5\n",
        "            if overall_categories['proprietary'] == 0:\n",
        "                logger.warning(\"\\n⚠️  WARNING: No proprietary features in overall top 5!\")\n",
        "\n",
        "            # Calculate overall feature importance\n",
        "            shap_importance = np.abs(shap_values).mean(axis=0)\n",
        "            feature_importance_df = pd.DataFrame({\n",
        "                'feature': self.feature_columns[prediction_days],\n",
        "                'shap_importance': shap_importance\n",
        "            }).sort_values('shap_importance', ascending=False)\n",
        "\n",
        "            # Store diagnostics\n",
        "            self.training_diagnostics[prediction_days] = {\n",
        "                'feature_importance': feature_importance_df,\n",
        "                'feature_importance_by_type': feature_importance_by_type,\n",
        "                'train_metrics': train_metrics,\n",
        "                'test_metrics': test_metrics,\n",
        "                'all_features': self.feature_columns[prediction_days],\n",
        "                'feature_categories': feature_categories\n",
        "            }\n",
        "\n",
        "            logger.info(\"SHAP explainer initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SHAP initialization failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        self.models[prediction_days] = rf_model\n",
        "\n",
        "        # Calculate per-stock metrics\n",
        "        logger.info(\"\\nCalculating per-stock test metrics and feature importance...\")\n",
        "        self.per_stock_metrics[prediction_days] = self.calculate_per_stock_metrics(stock_data, prediction_days)\n",
        "        logger.info(f\"Calculated metrics for {len(self.per_stock_metrics[prediction_days])} stocks\")\n",
        "\n",
        "        return rf_model\n",
        "\n",
        "    def predict_proba(self, features, prediction_days):\n",
        "        \"\"\"Make prediction with proper feature alignment\"\"\"\n",
        "        if prediction_days not in self.models:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            model = self.models[prediction_days]\n",
        "            scaler = self.scalers[prediction_days]\n",
        "            feature_cols = self.feature_columns[prediction_days]\n",
        "\n",
        "            if not isinstance(features, pd.DataFrame):\n",
        "                return None\n",
        "\n",
        "            # Create a new DataFrame with all required features\n",
        "            features_aligned = pd.DataFrame(index=features.index, columns=feature_cols)\n",
        "\n",
        "            # Fill in available features\n",
        "            for col in feature_cols:\n",
        "                if col in features.columns:\n",
        "                    features_aligned[col] = features[col]\n",
        "                else:\n",
        "                    features_aligned[col] = 0\n",
        "\n",
        "            # Fill any remaining NaN\n",
        "            features_aligned = features_aligned.fillna(0)\n",
        "\n",
        "            # Convert to numpy array for scaling\n",
        "            features_array = features_aligned.values\n",
        "\n",
        "            # Scale features\n",
        "            features_scaled = scaler.transform(features_array)\n",
        "\n",
        "            # Get prediction\n",
        "            proba = model.predict_proba(features_scaled)[0]\n",
        "            return proba\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in prediction: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_signal_shap_explanation(self, features, prediction_days):\n",
        "        \"\"\"Get enhanced SHAP explanation showing all feature types\"\"\"\n",
        "        if prediction_days not in self.models or prediction_days not in self.shap_explainers:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Prepare features\n",
        "            feature_cols = self.feature_columns[prediction_days]\n",
        "            scaler = self.scalers[prediction_days]\n",
        "\n",
        "            if not isinstance(features, pd.DataFrame):\n",
        "                return None\n",
        "\n",
        "            # Create aligned features DataFrame\n",
        "            features_aligned = pd.DataFrame(index=features.index, columns=feature_cols)\n",
        "\n",
        "            # Fill in available features\n",
        "            for col in feature_cols:\n",
        "                if col in features.columns:\n",
        "                    features_aligned[col] = features[col]\n",
        "                else:\n",
        "                    features_aligned[col] = 0\n",
        "\n",
        "            # Fill NaN and convert to array\n",
        "            features_aligned = features_aligned.fillna(0)\n",
        "            features_array = features_aligned.values\n",
        "\n",
        "            if features_array.shape[0] != 1:\n",
        "                features_array = features_array[0:1]\n",
        "\n",
        "            # Scale features\n",
        "            features_scaled = scaler.transform(features_array)\n",
        "\n",
        "            # Get prediction\n",
        "            proba = self.models[prediction_days].predict_proba(features_scaled)[0]\n",
        "\n",
        "            # Get SHAP values\n",
        "            try:\n",
        "                shap_values = self.shap_explainers[prediction_days].shap_values(features_scaled)\n",
        "            except:\n",
        "                explanation = self.shap_explainers[prediction_days](features_scaled)\n",
        "                shap_values = explanation.values\n",
        "\n",
        "            # Handle binary classification\n",
        "            if isinstance(shap_values, list) and len(shap_values) == 2:\n",
        "                shap_values = shap_values[1]\n",
        "            elif len(shap_values.shape) == 3:\n",
        "                shap_values = shap_values[:, :, 1]\n",
        "\n",
        "            # Ensure we have a 1D array\n",
        "            if shap_values.shape[0] == 1:\n",
        "                shap_values = shap_values[0]\n",
        "\n",
        "            # Get top features by type\n",
        "            top_features_by_type = self.get_top_features_by_type(\n",
        "                feature_cols, shap_values, n_per_type=5\n",
        "            )\n",
        "\n",
        "            # Create explanation object\n",
        "            explanation = {\n",
        "                'prob_up': proba[1],\n",
        "                'shap_values': shap_values,\n",
        "                'feature_names': feature_cols,\n",
        "                'feature_values': features_array[0],\n",
        "                'top_features_by_type': top_features_by_type\n",
        "            }\n",
        "\n",
        "            return explanation\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting SHAP explanation: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_feature_presence_heatmap(self, output_path=None):\n",
        "        \"\"\"Create heatmap showing feature type presence in top 5 for each stock\"\"\"\n",
        "        if not self.feature_presence_matrix:\n",
        "            logger.warning(\"No feature presence data available for heatmap\")\n",
        "            return\n",
        "\n",
        "        for horizon in CONFIG['HORIZONS']:\n",
        "            if horizon not in self.feature_presence_matrix:\n",
        "                continue\n",
        "\n",
        "            presence_data = self.feature_presence_matrix[horizon]\n",
        "\n",
        "            # Create presence matrix\n",
        "            stocks = list(presence_data.keys())\n",
        "            feature_types = ['macro', 'proprietary', 'technical', 'interaction', 'regime', 'transformed']\n",
        "\n",
        "            presence_matrix = pd.DataFrame(\n",
        "                index=stocks,\n",
        "                columns=feature_types,\n",
        "                data=0\n",
        "            )\n",
        "\n",
        "            for stock, type_counts in presence_data.items():\n",
        "                for feat_type, count in type_counts.items():\n",
        "                    if feat_type in presence_matrix.columns:\n",
        "                        presence_matrix.loc[stock, feat_type] = count\n",
        "\n",
        "            # Create heatmap\n",
        "            plt.figure(figsize=(10, max(8, len(stocks) * 0.3)))\n",
        "\n",
        "            # Create color map - 0 is red, >0 is green scale\n",
        "            colors = ['red', 'yellow', 'lightgreen', 'green', 'darkgreen']\n",
        "            n_colors = len(colors)\n",
        "            cmap = plt.cm.colors.ListedColormap(colors)\n",
        "            bounds = [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5]\n",
        "            norm = plt.cm.colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "            sns.heatmap(\n",
        "                presence_matrix.astype(float),\n",
        "                cmap=cmap,\n",
        "                norm=norm,\n",
        "                annot=True,\n",
        "                fmt='g',\n",
        "                cbar_kws={'label': 'Features in Top 5', 'ticks': [0, 1, 2, 3, 4]},\n",
        "                linewidths=0.5,\n",
        "                linecolor='gray'\n",
        "            )\n",
        "\n",
        "            plt.title(f'Feature Type Presence in Top 5 - {horizon}d Horizon\\n(Red = 0 features, Green = multiple features)')\n",
        "            plt.xlabel('Feature Type')\n",
        "            plt.ylabel('Stock')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            if output_path:\n",
        "                filename = f\"{output_path}_presence_{horizon}d.png\"\n",
        "                plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "                logger.info(f\"Saved feature presence heatmap to {filename}\")\n",
        "            else:\n",
        "                plt.show()\n",
        "\n",
        "            plt.close()\n",
        "\n",
        "            # Print summary statistics\n",
        "            logger.info(f\"\\n=== Feature Type Presence Summary for {horizon}d ===\")\n",
        "            for feat_type in feature_types:\n",
        "                stocks_with_type = (presence_matrix[feat_type] > 0).sum()\n",
        "                avg_count = presence_matrix[feat_type].mean()\n",
        "                logger.info(f\"{feat_type}: {stocks_with_type}/{len(stocks)} stocks ({stocks_with_type/len(stocks)*100:.1f}%), avg {avg_count:.2f} features\")\n",
        "\n",
        "            # Identify stocks missing proprietary features\n",
        "            stocks_missing_prop = presence_matrix[presence_matrix['proprietary'] == 0].index.tolist()\n",
        "            if stocks_missing_prop:\n",
        "                logger.warning(f\"\\nStocks with NO proprietary features in top 5: {stocks_missing_prop}\")\n",
        "\n",
        "# ==========================\n",
        "# SIGNAL GENERATION\n",
        "# ==========================\n",
        "\n",
        "def generate_signals_with_shap(stock_data, ml_model, macro_metadata, timeframe=30):\n",
        "    \"\"\"Generate trading signals with comprehensive SHAP explanations\"\"\"\n",
        "    signals = []\n",
        "\n",
        "    # Track statistics\n",
        "    feature_type_counts = defaultdict(int)\n",
        "    successful_signals = 0\n",
        "    failed_signals = 0\n",
        "\n",
        "    # Calculate market metrics\n",
        "    market_metrics = {}\n",
        "    for ticker, df in stock_data.items():\n",
        "        if len(df) < 60:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            current_price = float(df['Close'].iloc[-1])\n",
        "            sma20_series = df['Close'].rolling(20).mean()\n",
        "            sma20 = float(sma20_series.iloc[-1]) if not pd.isna(sma20_series.iloc[-1]) else current_price\n",
        "\n",
        "            # Calculate momentum\n",
        "            price_20d_ago = float(df['Close'].iloc[-20])\n",
        "            momentum_20d = (current_price / price_20d_ago - 1) * 100 if price_20d_ago > 0 else 0.0\n",
        "\n",
        "            # Calculate RSI\n",
        "            rsi_series = calculate_rsi(df['Close'])\n",
        "            rsi_val = float(rsi_series.iloc[-1]) if not pd.isna(rsi_series.iloc[-1]) else 50.0\n",
        "\n",
        "            market_metrics[ticker] = {\n",
        "                'momentum_20d': momentum_20d,\n",
        "                'rsi': rsi_val,\n",
        "                'above_sma20': current_price > sma20,\n",
        "                'df': df,\n",
        "                'current_date': df['Date'].iloc[-1] if 'Date' in df.columns else pd.Timestamp.now()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating metrics for {ticker}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not market_metrics:\n",
        "        logger.warning(\"No valid market metrics calculated\")\n",
        "        return signals\n",
        "\n",
        "    # Calculate market regime\n",
        "    stocks_above_sma20 = sum(1 for m in market_metrics.values() if m['above_sma20'])\n",
        "    market_breadth = stocks_above_sma20 / len(market_metrics)\n",
        "\n",
        "    if market_breadth > 0.65:\n",
        "        regime = 'BULL'\n",
        "    elif market_breadth < 0.35:\n",
        "        regime = 'BEAR'\n",
        "    else:\n",
        "        regime = 'NEUTRAL'\n",
        "\n",
        "    logger.info(f\"\\nGenerating Signals for {timeframe}-day horizon\")\n",
        "    logger.info(f\"  Market breadth: {market_breadth:.1%}\")\n",
        "    logger.info(f\"  Market regime: {regime}\")\n",
        "    logger.info(f\"  Processing {len(market_metrics)} stocks...\")\n",
        "\n",
        "    # Generate signal for each stock\n",
        "    for ticker, metrics in market_metrics.items():\n",
        "        df = metrics['df']\n",
        "        signal_date = metrics['current_date']\n",
        "\n",
        "        try:\n",
        "            # Create features - includes ALL proprietary features\n",
        "            features = create_all_features(df, macro_metadata)\n",
        "            if features.empty or len(features) == 0:\n",
        "                failed_signals += 1\n",
        "                continue\n",
        "\n",
        "            # Get the last row of features\n",
        "            last_features = features.iloc[-1:].copy()\n",
        "\n",
        "            # Get ML prediction\n",
        "            proba = ml_model.predict_proba(last_features, timeframe)\n",
        "            if proba is None:\n",
        "                failed_signals += 1\n",
        "                continue\n",
        "\n",
        "            prob_up = float(proba[1])\n",
        "\n",
        "            # Get SHAP explanation with all feature types\n",
        "            shap_explanation = ml_model.get_signal_shap_explanation(last_features, timeframe)\n",
        "\n",
        "            # Calculate ALL indicators\n",
        "            indicators = calculate_indicators(df)\n",
        "\n",
        "            # Process SHAP explanation\n",
        "            shap_features = []\n",
        "            shap_display = 'N/A'\n",
        "            top_features_by_type = None\n",
        "            feature_presence = defaultdict(int)\n",
        "            driver_type = 'Unknown'\n",
        "\n",
        "            if shap_explanation:\n",
        "                try:\n",
        "                    top_features_by_type = shap_explanation.get('top_features_by_type', {})\n",
        "\n",
        "                    # Get top 5 overall features\n",
        "                    overall_top_5 = top_features_by_type.get('overall_top_5', [])\n",
        "\n",
        "                    # Format features for display\n",
        "                    formatted_features = []\n",
        "\n",
        "                    for feat_name, importance, idx in overall_top_5:\n",
        "                        # Determine category\n",
        "                        categories = ml_model.categorize_features([feat_name])\n",
        "                        category = next(iter([k for k, v in categories.items() if v]), 'unknown')\n",
        "\n",
        "                        # Track feature presence\n",
        "                        feature_presence[category] += 1\n",
        "\n",
        "                        feat_value = shap_explanation['feature_values'][idx]\n",
        "                        shap_val = shap_explanation['shap_values'][idx]\n",
        "\n",
        "                        formatted = format_shap_feature_complete(feat_name, shap_val, feat_value, category)\n",
        "                        formatted_features.append(formatted)\n",
        "\n",
        "                        shap_features.append({\n",
        "                            'feature': feat_name,\n",
        "                            'shap_value': float(shap_val),\n",
        "                            'feature_type': category,\n",
        "                            'actual_value': float(feat_value),\n",
        "                            'rank': len(shap_features) + 1\n",
        "                        })\n",
        "\n",
        "                    shap_display = ' | '.join(formatted_features)\n",
        "\n",
        "                    # Determine driver type based on feature presence\n",
        "                    if feature_presence['proprietary'] >= 2:\n",
        "                        driver_type = 'Proprietary-driven'\n",
        "                    elif feature_presence['macro'] >= 2:\n",
        "                        driver_type = 'Macro-driven'\n",
        "                    elif feature_presence['interaction'] >= 2:\n",
        "                        driver_type = 'Interaction-driven'\n",
        "                    elif len(feature_presence) >= 3:\n",
        "                        driver_type = 'Mixed-diverse'\n",
        "                    else:\n",
        "                        driver_type = list(feature_presence.keys())[0] + '-driven' if feature_presence else 'Unknown'\n",
        "\n",
        "                    feature_type_counts[driver_type] += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing SHAP for {ticker}: {e}\")\n",
        "\n",
        "            # Calculate combination-based confidence\n",
        "            confidence = calculate_combination_confidence(prob_up, indicators, regime, top_features_by_type)\n",
        "\n",
        "            # Get per-stock performance metrics\n",
        "            if timeframe in ml_model.per_stock_metrics and ticker in ml_model.per_stock_metrics[timeframe]:\n",
        "                stock_metrics = ml_model.per_stock_metrics[timeframe][ticker]\n",
        "                accuracy = stock_metrics['accuracy']\n",
        "                sharpe = stock_metrics['sharpe_ratio']\n",
        "                win_rate = stock_metrics['win_rate']\n",
        "                max_drawdown = stock_metrics['max_drawdown']\n",
        "                cagr = stock_metrics.get('cagr', 0)\n",
        "            else:\n",
        "                # Fallback to overall model metrics\n",
        "                if timeframe in ml_model.performance_metrics:\n",
        "                    test_metrics = ml_model.performance_metrics[timeframe]['test']\n",
        "                    accuracy = test_metrics['accuracy']\n",
        "                    sharpe = test_metrics['sharpe_ratio']\n",
        "                    win_rate = test_metrics['win_rate']\n",
        "                    max_drawdown = test_metrics['max_drawdown']\n",
        "                    cagr = 0\n",
        "                else:\n",
        "                    accuracy = 50.0\n",
        "                    sharpe = 0.0\n",
        "                    win_rate = 50.0\n",
        "                    max_drawdown = 0.0\n",
        "                    cagr = 0.0\n",
        "\n",
        "            # Determine signal using combination logic\n",
        "            signal = determine_combination_signal(prob_up, confidence, indicators, top_features_by_type)\n",
        "\n",
        "            # Determine direction\n",
        "            direction = 'UP' if prob_up > 0.5 else 'DOWN'\n",
        "\n",
        "            # Build comprehensive signal data\n",
        "            signal_data = {\n",
        "                'ticker': ticker,\n",
        "                'Stock': ticker,\n",
        "                'horizon': f'{timeframe}d',\n",
        "                'signal': signal,\n",
        "                'Signal': signal,\n",
        "                'direction': direction,\n",
        "                'Price_Direction': direction.title(),\n",
        "                'confidence': float(confidence),\n",
        "                'Confidence': float(confidence),\n",
        "                'prob_up': float(prob_up),\n",
        "                'accuracy': float(accuracy),\n",
        "                'Accuracy': float(accuracy),\n",
        "                'sharpe_ratio': float(sharpe),\n",
        "                'Sharpe': float(sharpe),\n",
        "                'win_rate': float(win_rate),\n",
        "                'max_drawdown': float(max_drawdown),\n",
        "                'Drawdown': float(max_drawdown),\n",
        "                'shap_top_5': shap_display,\n",
        "                'SHAP': shap_display,\n",
        "                'driver_type': driver_type,\n",
        "                'indicators': indicators,\n",
        "                'regime': regime,\n",
        "                'signal_date': signal_date,\n",
        "                'shap_features': shap_features,\n",
        "                'top_features_by_type': top_features_by_type,\n",
        "                'feature_presence': dict(feature_presence),\n",
        "\n",
        "                # ALL fields including proprietary\n",
        "                'CAGR': float(cagr),\n",
        "                'VIX': float(indicators.get('VIX', 0)),\n",
        "                'FNG': float(indicators.get('FNG', 0)),\n",
        "                'RSI': float(indicators.get('RSI', 50)),\n",
        "                'AnnVolatility': float(indicators.get('AnnVolatility', 30)),\n",
        "                'Momentum125': float(indicators.get('Momentum125', 0)),\n",
        "                'PriceStrength': float(indicators.get('PriceStrength', 0)),\n",
        "                'VolumeBreadth': float(indicators.get('VolumeBreadth', 1)),\n",
        "                'CallPut': float(indicators.get('CallPut', 50)),\n",
        "                'NewsScore': float(indicators.get('NewsScore', 5)),\n",
        "                'MACD': float(indicators.get('MACD', 0)),\n",
        "                'BollingerBandWidth': float(indicators.get('BollingerBandWidth', 2)),\n",
        "                'ATR': float(indicators.get('ATR', 1)),\n",
        "                'StochRSI': float(indicators.get('StochRSI', 50)),\n",
        "                'OBV': float(indicators.get('OBV', 0)),\n",
        "                'CMF': float(indicators.get('CMF', 0)),\n",
        "                'ADX': float(indicators.get('ADX', 25)),\n",
        "                'Williams_R': float(indicators.get('Williams_R', -50)),\n",
        "                'CCI': float(indicators.get('CCI', 0)),\n",
        "                'MFI': float(indicators.get('MFI', 50)),\n",
        "\n",
        "                # Price data\n",
        "                'Current_Price': float(df['Close'].iloc[-1]),\n",
        "                'SMA20': float(df['Close'].rolling(20).mean().iloc[-1]) if len(df) >= 20 else float(df['Close'].iloc[-1]),\n",
        "                'Vol_Breadth': float(indicators.get('VolumeBreadth', 1.0)),\n",
        "                'BL20': 0.0,\n",
        "                'BH20': 0.0,\n",
        "            }\n",
        "\n",
        "            # Calculate Bollinger Bands\n",
        "            if len(df) >= 20:\n",
        "                bb_data = calculate_bollinger_bands(df['Close'])\n",
        "                signal_data['BL20'] = float(bb_data['lower'].iloc[-1])\n",
        "                signal_data['BH20'] = float(bb_data['upper'].iloc[-1])\n",
        "\n",
        "            # Create IF/THEN logic\n",
        "            if_then_logic = create_if_then_logic_complete(\n",
        "                ticker,\n",
        "                timeframe,\n",
        "                direction.title(),\n",
        "                signal,\n",
        "                accuracy,\n",
        "                shap_features,\n",
        "                indicators,\n",
        "                sharpe,\n",
        "                float(df['Close'].iloc[-1]),\n",
        "                signal_data['BL20'],\n",
        "                signal_data['BH20'],\n",
        "                feature_presence\n",
        "            )\n",
        "            signal_data['IF_THEN'] = if_then_logic\n",
        "\n",
        "            signals.append(signal_data)\n",
        "            successful_signals += 1\n",
        "\n",
        "            # Update model statistics\n",
        "            ml_model.signal_statistics[timeframe]['total'] += 1\n",
        "            ml_model.signal_statistics[timeframe][driver_type.lower()] += 1\n",
        "            ml_model.signal_statistics[timeframe][signal.lower().replace(' ', '_')] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing {ticker}: {str(e)[:100]}\")\n",
        "            failed_signals += 1\n",
        "            continue\n",
        "\n",
        "    # Print summary\n",
        "    logger.info(f\"\\nSignal Generation Summary ({timeframe}d):\")\n",
        "    logger.info(f\"  Successful: {successful_signals}\")\n",
        "    logger.info(f\"  Failed: {failed_signals}\")\n",
        "\n",
        "    if len(signals) > 0:\n",
        "        # Signal distribution\n",
        "        signal_counts = defaultdict(int)\n",
        "        for s in signals:\n",
        "            signal_counts[s['signal']] += 1\n",
        "\n",
        "        logger.info(f\"\\nSignal Distribution:\")\n",
        "        for signal_type, count in signal_counts.items():\n",
        "            logger.info(f\"  {signal_type}: {count} ({count/len(signals)*100:.1f}%)\")\n",
        "\n",
        "        # Driver type distribution\n",
        "        logger.info(f\"\\nDriver Type Distribution:\")\n",
        "        total_counts = sum(feature_type_counts.values())\n",
        "        if total_counts > 0:\n",
        "            for driver_type, count in sorted(feature_type_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "                logger.info(f\"  {driver_type}: {count} ({count/total_counts*100:.1f}%)\")\n",
        "\n",
        "        # Feature presence summary\n",
        "        feature_presence_summary = defaultdict(int)\n",
        "        for signal in signals:\n",
        "            if 'feature_presence' in signal:\n",
        "                for feat_type, count in signal['feature_presence'].items():\n",
        "                    if count > 0:\n",
        "                        feature_presence_summary[feat_type] += 1\n",
        "\n",
        "        logger.info(f\"\\nFeature Types in Top 5 (across all stocks):\")\n",
        "        for feat_type, count in sorted(feature_presence_summary.items(), key=lambda x: x[1], reverse=True):\n",
        "            logger.info(f\"  {feat_type}: {count}/{len(signals)} stocks ({count/len(signals)*100:.1f}%)\")\n",
        "\n",
        "    return signals\n",
        "\n",
        "# ==========================\n",
        "# DISPLAY FUNCTIONS\n",
        "# ==========================\n",
        "\n",
        "def get_performance_indicator(value: float, metric_type: str) -> str:\n",
        "    \"\"\"Return a colored emoji representing the value.\"\"\"\n",
        "    if pd.isna(value) or value is None:\n",
        "        return \"\"\n",
        "\n",
        "    if metric_type == \"accuracy\":\n",
        "        return \"🟢\" if value >= 65 else \"🟡\" if value >= 55 else \"🔴\"\n",
        "    elif metric_type == \"sharpe\":\n",
        "        return \"🟢\" if value >= 1.0 else \"🟡\" if value >= 0.5 else \"🔴\"\n",
        "    elif metric_type == \"cagr\":\n",
        "        return \"🟢\" if value >= 20 else \"🟡\" if value >= 10 else \"🔴\"\n",
        "    elif metric_type == \"vix\":\n",
        "        if value == 0: return \"\"\n",
        "        return \"🔴\" if value > 35 else \"🟡\" if value > 25 else \"🟢\"\n",
        "    elif metric_type == \"call_put\":\n",
        "        if value == 0: return \"\"\n",
        "        return \"🟢\" if value > 60 else \"🔴\" if value < 40 else \"🟡\" if value > 55 or value < 45 else \"⚪\"\n",
        "    elif metric_type == \"fng\":\n",
        "        if value == 0: return \"\"\n",
        "        return \"🟢\" if value > 75 else \"🟢\" if value > 60 else \"🔴\" if value < 25 else \"🔴\" if value < 40 else \"⚪\"\n",
        "    elif metric_type == \"news\":\n",
        "        if value == 0: return \"\"\n",
        "        return \"🟢\" if value >= 7 else \"🟡\" if value >= 5 else \"🔴\"\n",
        "    elif metric_type == \"volatility\":\n",
        "        if value == 0: return \"\"\n",
        "        return \"🔴\" if value > 40 else \"🟡\" if value > 30 else \"🟢\"\n",
        "    elif metric_type == \"momentum\":\n",
        "        return \"🟢\" if value > 20 else \"🟡\" if value > 10 else \"🔴\" if value < -10 else \"⚪\"\n",
        "    return \"\"\n",
        "\n",
        "def create_complete_playbook_tables(df, horizon):\n",
        "    \"\"\"Create complete tables showing ALL data including missing values\"\"\"\n",
        "    if len(df) == 0:\n",
        "        return\n",
        "\n",
        "    # Sort by accuracy\n",
        "    df = df.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "    print(f\"\\n{'='*150}\")\n",
        "    print(f\"<span style='font-size:24px;font-weight:bold'>📊 {horizon}-DAY COMPLETE ANALYSIS</span>\")\n",
        "    print(f\"{'='*150}\")\n",
        "\n",
        "    # Prepare COMPLETE data table with ALL proprietary features\n",
        "    complete_rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        # Get all proprietary values\n",
        "        prop_values = {}\n",
        "        for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "            value = row.get(feat, np.nan)\n",
        "            prop_values[feat] = display_value(value)\n",
        "\n",
        "        complete_rows.append({\n",
        "            \"Ticker\": row['Stock'],\n",
        "            \"Signal\": row['Signal'],\n",
        "            \"Accuracy\": f\"{get_performance_indicator(row['Accuracy'], 'accuracy')}{row['Accuracy']:.1f}%\",\n",
        "            \"Sharpe\": f\"{get_performance_indicator(row['Sharpe'], 'sharpe')}{row['Sharpe']:.2f}\",\n",
        "            \"CAGR\": f\"{get_performance_indicator(row['CAGR'], 'cagr')}{display_value(row['CAGR'])}%\",\n",
        "            \"VIX\": f\"{get_performance_indicator(row['VIX'], 'vix')}{prop_values['VIX']}\",\n",
        "            \"FNG\": f\"{get_performance_indicator(row['FNG'], 'fng')}{prop_values['FNG']}\",\n",
        "            \"RSI\": prop_values['RSI'],\n",
        "            \"AnnVol\": f\"{get_performance_indicator(row['AnnVolatility'], 'volatility')}{prop_values['AnnVolatility']}%\",\n",
        "            \"Mom125\": f\"{get_performance_indicator(row['Momentum125'], 'momentum')}{prop_values['Momentum125']}%\",\n",
        "            \"PriceStr\": prop_values['PriceStrength'],\n",
        "            \"VolBreadth\": prop_values['VolumeBreadth'],\n",
        "            \"MACD\": prop_values['MACD'],\n",
        "            \"ATR\": prop_values['ATR'],\n",
        "            \"ADX\": prop_values['ADX'],\n",
        "            \"StochRSI\": prop_values['StochRSI'],\n",
        "            \"CCI\": prop_values['CCI'],\n",
        "            \"MFI\": prop_values['MFI'],\n",
        "            \"News\": prop_values['NewsScore'],\n",
        "            \"CallPut\": prop_values['CallPut']\n",
        "        })\n",
        "\n",
        "    complete_df = pd.DataFrame(complete_rows)\n",
        "\n",
        "    # Print COMPLETE proprietary features table\n",
        "    print(f\"\\n📈 **{horizon}-DAY COMPLETE PROPRIETARY FEATURES TABLE**\")\n",
        "    print(\"```\")\n",
        "    print(complete_df.to_markdown(index=False))\n",
        "    print(\"```\")\n",
        "\n",
        "    # Prepare SHAP analysis table\n",
        "    shap_rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        # Parse feature presence\n",
        "        feature_presence = row.get('feature_presence', {})\n",
        "        presence_str = \", \".join([f\"{k}:{v}\" for k, v in feature_presence.items() if v > 0])\n",
        "\n",
        "        shap_rows.append({\n",
        "            \"Ticker\": row['Stock'],\n",
        "            \"Top 5 SHAP Features\": row.get('SHAP', 'N/A'),\n",
        "            \"Feature Types Present\": presence_str or \"None\",\n",
        "            \"Driver\": row.get('driver_type', 'Unknown')\n",
        "        })\n",
        "\n",
        "    shap_df = pd.DataFrame(shap_rows)\n",
        "\n",
        "    # Print SHAP analysis table\n",
        "    print(f\"\\n🔍 **{horizon}-DAY SHAP FEATURE ANALYSIS**\")\n",
        "    print(\"```\")\n",
        "    print(shap_df.to_markdown(index=False))\n",
        "    print(\"```\")\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"\\n📊 **{horizon}-DAY SUMMARY STATISTICS**\")\n",
        "\n",
        "    # Calculate statistics for ALL proprietary features\n",
        "    prop_stats = {}\n",
        "    for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "        if feat in df.columns:\n",
        "            values = df[feat].dropna()\n",
        "            if len(values) > 0:\n",
        "                prop_stats[feat] = {\n",
        "                    'mean': values.mean(),\n",
        "                    'std': values.std(),\n",
        "                    'min': values.min(),\n",
        "                    'max': values.max(),\n",
        "                    'coverage': len(values) / len(df) * 100\n",
        "                }\n",
        "\n",
        "    # Print proprietary feature statistics\n",
        "    print(\"\\n**Proprietary Feature Statistics:**\")\n",
        "    print(f\"{'Feature':<15} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10} {'Coverage':>10}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for feat, stats in prop_stats.items():\n",
        "        print(f\"{feat:<15} {stats['mean']:>10.1f} {stats['std']:>10.1f} \"\n",
        "              f\"{stats['min']:>10.1f} {stats['max']:>10.1f} {stats['coverage']:>9.1f}%\")\n",
        "\n",
        "    # Feature type presence in top 5\n",
        "    print(\"\\n**Feature Type Presence in Top 5 SHAP:**\")\n",
        "    type_counts = defaultdict(int)\n",
        "    for _, row in df.iterrows():\n",
        "        if 'feature_presence' in row:\n",
        "            for feat_type, count in row['feature_presence'].items():\n",
        "                if count > 0:\n",
        "                    type_counts[feat_type] += 1\n",
        "\n",
        "    for feat_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        pct = count / len(df) * 100\n",
        "        print(f\"  {feat_type}: {count}/{len(df)} stocks ({pct:.1f}%)\")\n",
        "\n",
        "    # Warning if proprietary features are underrepresented\n",
        "    if type_counts.get('proprietary', 0) < len(df) * 0.3:\n",
        "        print(\"\\n⚠️  WARNING: Proprietary features appear in less than 30% of top 5 SHAP features!\")\n",
        "\n",
        "    # IF/THEN examples\n",
        "    print(f\"\\n💡 **{horizon}-DAY IF/THEN LOGIC EXAMPLES**\")\n",
        "\n",
        "    example_count = 0\n",
        "    for _, row in df.iterrows():\n",
        "        if example_count >= 5:\n",
        "            break\n",
        "\n",
        "        if_then = row.get('IF_THEN', '')\n",
        "        if if_then and if_then != 'N/A':\n",
        "            print(f\"\\n{if_then}\")\n",
        "            example_count += 1\n",
        "\n",
        "def analyze_feature_diversity_complete(all_signals, ml_model):\n",
        "    \"\"\"Complete analysis of feature diversity with detailed breakdown\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*150)\n",
        "    print(\"**COMPLETE FEATURE DIVERSITY ANALYSIS**\")\n",
        "    print(\"=\"*150)\n",
        "\n",
        "    # Detailed tracking\n",
        "    feature_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "    feature_values = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
        "    stocks_with_feature = defaultdict(lambda: defaultdict(set))\n",
        "    total_stocks_per_horizon = defaultdict(int)\n",
        "\n",
        "    # Track proprietary feature coverage\n",
        "    proprietary_coverage = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for signal in all_signals:\n",
        "        horizon = int(signal['horizon'].replace('d', ''))\n",
        "        ticker = signal['ticker']\n",
        "        total_stocks_per_horizon[horizon] += 1\n",
        "\n",
        "        # Track proprietary feature values\n",
        "        for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "            if feat in signal and signal[feat] != 0:\n",
        "                proprietary_coverage[horizon][feat] += 1\n",
        "\n",
        "        # Track SHAP features\n",
        "        if 'shap_features' in signal and signal['shap_features']:\n",
        "            for feat_info in signal['shap_features']:\n",
        "                feat_type = feat_info.get('feature_type', 'unknown')\n",
        "                feat_name = feat_info['feature']\n",
        "\n",
        "                feature_counts[horizon][feat_type][feat_name] += 1\n",
        "                feature_values[horizon][feat_type][feat_name].append(abs(feat_info['shap_value']))\n",
        "                stocks_with_feature[horizon][feat_type].add(ticker)\n",
        "\n",
        "    # Print proprietary feature coverage\n",
        "    print(\"\\n📊 **PROPRIETARY FEATURE DATA COVERAGE**\")\n",
        "    print(\"\\n{:<20} {:>15} {:>15} {:>15}\".format(\"Feature\", \"30d Coverage\", \"45d Coverage\", \"60d Coverage\"))\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "        coverages = []\n",
        "        for h in CONFIG['HORIZONS']:\n",
        "            count = proprietary_coverage[h].get(feat, 0)\n",
        "            total = total_stocks_per_horizon[h]\n",
        "            pct = (count / total * 100) if total > 0 else 0\n",
        "            coverages.append(f\"{count}/{total} ({pct:.1f}%)\")\n",
        "\n",
        "        print(\"{:<20} {:>15} {:>15} {:>15}\".format(feat, *coverages))\n",
        "\n",
        "    # Detailed feature type analysis\n",
        "    print(\"\\n📊 **FEATURE TYPE REPRESENTATION IN TOP 5 SHAP**\")\n",
        "\n",
        "    for feat_type in ['proprietary', 'macro', 'technical', 'interaction', 'regime', 'transformed']:\n",
        "        print(f\"\\n--- {feat_type.upper()} Features ---\")\n",
        "        print(\"{:<40} {:>12} {:>12} {:>12}\".format(\"Feature\", \"30d Count\", \"45d Count\", \"60d Count\"))\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Get all features of this type\n",
        "        all_features = set()\n",
        "        for horizon in CONFIG['HORIZONS']:\n",
        "            all_features.update(feature_counts[horizon][feat_type].keys())\n",
        "\n",
        "        if not all_features:\n",
        "            print(f\"  NO {feat_type} features found in top 5 SHAP!\")\n",
        "            continue\n",
        "\n",
        "        # Sort by total frequency\n",
        "        feature_totals = {}\n",
        "        for feature in all_features:\n",
        "            total = sum(feature_counts[h][feat_type].get(feature, 0) for h in CONFIG['HORIZONS'])\n",
        "            feature_totals[feature] = total\n",
        "\n",
        "        # Print top features\n",
        "        for feature in sorted(all_features, key=lambda x: feature_totals[x], reverse=True)[:10]:\n",
        "            counts = [feature_counts[h][feat_type].get(feature, 0) for h in CONFIG['HORIZONS']]\n",
        "            print(\"{:<40} {:>12} {:>12} {:>12}\".format(\n",
        "                feature[:40], counts[0], counts[1], counts[2]\n",
        "            ))\n",
        "\n",
        "    # Stock-level analysis\n",
        "    print(\"\\n📊 **STOCK-LEVEL FEATURE TYPE COVERAGE**\")\n",
        "\n",
        "    for horizon in CONFIG['HORIZONS']:\n",
        "        print(f\"\\n{horizon}-day horizon:\")\n",
        "        total_stocks = total_stocks_per_horizon[horizon]\n",
        "\n",
        "        for feat_type in ['proprietary', 'macro', 'technical', 'interaction']:\n",
        "            stocks_with_type = len(stocks_with_feature[horizon][feat_type])\n",
        "            pct = (stocks_with_type / total_stocks * 100) if total_stocks > 0 else 0\n",
        "            print(f\"  {feat_type}: {stocks_with_type}/{total_stocks} stocks ({pct:.1f}%)\")\n",
        "\n",
        "    # Critical warnings\n",
        "    print(\"\\n⚠️  **CRITICAL WARNINGS**\")\n",
        "\n",
        "    warnings_found = False\n",
        "\n",
        "    # Check proprietary feature representation\n",
        "    for horizon in CONFIG['HORIZONS']:\n",
        "        prop_stocks = len(stocks_with_feature[horizon]['proprietary'])\n",
        "        total_stocks = total_stocks_per_horizon[horizon]\n",
        "\n",
        "        if total_stocks > 0 and prop_stocks / total_stocks < 0.5:\n",
        "            print(f\"\\n⚠️  {horizon}d: Only {prop_stocks}/{total_stocks} stocks \"\n",
        "                  f\"({prop_stocks/total_stocks*100:.1f}%) have proprietary features in top 5!\")\n",
        "            warnings_found = True\n",
        "\n",
        "    # Check if specific proprietary features are missing\n",
        "    missing_proprietary = set()\n",
        "    for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "        found = False\n",
        "        for horizon in CONFIG['HORIZONS']:\n",
        "            if any(feat in fname for fname in feature_counts[horizon]['proprietary'].keys()):\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            missing_proprietary.add(feat)\n",
        "\n",
        "    if missing_proprietary:\n",
        "        print(f\"\\n⚠️  These proprietary features NEVER appear in top 5 SHAP: {missing_proprietary}\")\n",
        "        warnings_found = True\n",
        "\n",
        "    if not warnings_found:\n",
        "        print(\"\\n✅ No critical warnings - feature diversity appears healthy\")\n",
        "\n",
        "    # Recommendations\n",
        "    print(\"\\n💡 **SPECIFIC RECOMMENDATIONS**\")\n",
        "\n",
        "    print(\"\\n1. **To Improve Proprietary Feature Visibility:**\")\n",
        "    print(\"   - Increase interaction terms between low-visibility proprietary features and high-impact macro features\")\n",
        "    print(\"   - Add polynomial features for key proprietary indicators (VIX², FNG×RSI, etc.)\")\n",
        "    print(\"   - Consider feature engineering specific to regime changes\")\n",
        "\n",
        "    print(\"\\n2. **Model Architecture Adjustments:**\")\n",
        "    print(\"   - Set min_samples_leaf lower (try 5) to capture proprietary feature nuances\")\n",
        "    print(\"   - Use feature_importances_ to pre-select diverse features\")\n",
        "    print(\"   - Consider ensemble with proprietary-focused and macro-focused models\")\n",
        "\n",
        "    print(\"\\n3. **Data Quality Improvements:**\")\n",
        "    print(\"   - Ensure proprietary features have sufficient variation\")\n",
        "    print(\"   - Check for multicollinearity between similar indicators\")\n",
        "    print(\"   - Validate proprietary feature calculations\")\n",
        "\n",
        "# ==========================\n",
        "# MAIN EXECUTION\n",
        "# ==========================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with complete feature integration\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ENHANCED TRADING SYSTEM - COMPLETE PROPRIETARY INTEGRATION\")\n",
        "    print(\"LIMITED TO 5 STOCKS FOR COLAB MEMORY EFFICIENCY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Check if running in Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "        print(\"Running in Google Colab\")\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        from google.colab import drive\n",
        "        import os\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            print(\"Mounting Google Drive...\")\n",
        "            drive.mount('/content/drive')\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "        print(\"Not running in Google Colab\")\n",
        "\n",
        "    try:\n",
        "        # Get all CSV files from directory\n",
        "        all_csv_files = []\n",
        "        if os.path.exists(CONFIG['STOCK_DATA_PATH']):\n",
        "            all_csv_files = [f for f in os.listdir(CONFIG['STOCK_DATA_PATH'])\n",
        "                            if f.endswith('.csv') and not f.endswith('.gsheet.csv')]\n",
        "\n",
        "        # Get tickers for ONLY 5 CSV files\n",
        "        all_tickers = []\n",
        "        for filename in all_csv_files[:CONFIG['MAX_STOCKS']]:  # Process only 5 stocks\n",
        "            stock_id = filename.replace('.csv', '')\n",
        "            if stock_id in STOCK_ID_TO_NAME:\n",
        "                ticker = STOCK_ID_TO_NAME[stock_id]\n",
        "            else:\n",
        "                ticker = None\n",
        "                for tick, ids in STOCK_ALTERNATIVE_NAMES.items():\n",
        "                    if stock_id == tick:\n",
        "                        ticker = tick\n",
        "                        break\n",
        "                if not ticker:\n",
        "                    ticker = stock_id\n",
        "            all_tickers.append(ticker)\n",
        "\n",
        "        print(f\"\\nProcessing {len(all_tickers)} stocks: {', '.join(all_tickers)}\")\n",
        "\n",
        "        # Load stock data\n",
        "        print(\"\\n1. Loading stock data with ALL proprietary features...\")\n",
        "        stock_data = load_all_stock_data(all_tickers)\n",
        "\n",
        "        if not stock_data:\n",
        "            print(\"ERROR: No stock data loaded. Check file paths and stock IDs.\")\n",
        "            return\n",
        "\n",
        "        print(f\"   Loaded {len(stock_data)} stocks\")\n",
        "\n",
        "        # Complete proprietary feature coverage analysis\n",
        "        print(\"\\n📊 PROPRIETARY FEATURE COVERAGE IN RAW DATA:\")\n",
        "        proprietary_feature_coverage = defaultdict(int)\n",
        "        for ticker, df in stock_data.items():\n",
        "            for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "                if feat in df.columns:\n",
        "                    proprietary_feature_coverage[feat] += 1\n",
        "\n",
        "        print(f\"\\n{'Feature':<20} {'Coverage':>20} {'Percentage':>15}\")\n",
        "        print(\"-\" * 60)\n",
        "        for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "            count = proprietary_feature_coverage[feat]\n",
        "            pct = count / len(stock_data) * 100\n",
        "            status = \"✅\" if count > 0 else \"❌\"\n",
        "            print(f\"{feat:<20} {count}/{len(stock_data):>20} {pct:>14.1f}% {status}\")\n",
        "\n",
        "        # Load FRED data\n",
        "        print(\"\\n2. Loading FRED economic indicators...\")\n",
        "        fred_data_raw = load_fred_data_from_folders()\n",
        "\n",
        "        if not fred_data_raw:\n",
        "            print(\"WARNING: No FRED data loaded. Continuing with technical analysis only.\")\n",
        "            aligned_fred_data = {}\n",
        "        else:\n",
        "            print(f\"   Loaded {len(fred_data_raw)} raw indicators\")\n",
        "\n",
        "            # Align FRED data\n",
        "            print(\"\\n3. Aligning FRED data with proper lags...\")\n",
        "            aligned_fred_data = fix_macro_data_alignment(fred_data_raw)\n",
        "            print(f\"   Created {len(aligned_fred_data)} aligned indicators\")\n",
        "\n",
        "        # Merge data\n",
        "        print(\"\\n4. Merging macro data with stock data...\")\n",
        "        merged_stock_data, macro_metadata = merge_macro_with_stock(stock_data, aligned_fred_data)\n",
        "\n",
        "        # Initialize model\n",
        "        ml_model = EnhancedTradingModel()\n",
        "\n",
        "        # Store all signals\n",
        "        all_signals = []\n",
        "\n",
        "        # Train models and generate signals for each horizon\n",
        "        for horizon in CONFIG['HORIZONS']:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"PROCESSING {horizon}-DAY HORIZON\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            # Train model\n",
        "            print(f\"\\n5. Training ML model for {horizon}-day predictions...\")\n",
        "            model = ml_model.train_model(merged_stock_data, macro_metadata, horizon)\n",
        "\n",
        "            if model is None:\n",
        "                print(f\"ERROR: Model training failed for {horizon}-day horizon\")\n",
        "                continue\n",
        "\n",
        "            # Generate signals\n",
        "            print(f\"\\n6. Generating trading signals for {horizon}-day horizon...\")\n",
        "            signals = generate_signals_with_shap(merged_stock_data, ml_model, macro_metadata, horizon)\n",
        "\n",
        "            # Add to all signals\n",
        "            all_signals.extend(signals)\n",
        "\n",
        "        # Complete feature diversity analysis\n",
        "        print(\"\\n7. Analyzing complete feature diversity...\")\n",
        "        analyze_feature_diversity_complete(all_signals, ml_model)\n",
        "\n",
        "        # Create feature presence heatmaps\n",
        "        print(\"\\n8. Creating feature presence heatmaps...\")\n",
        "        if IN_COLAB:\n",
        "            heatmap_prefix = '/content/drive/MyDrive/feature_presence'\n",
        "        else:\n",
        "            heatmap_prefix = 'feature_presence'\n",
        "        ml_model.create_feature_presence_heatmap(heatmap_prefix)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*150)\n",
        "        print(\"**COMPREHENSIVE TRADING SIGNAL ANALYSIS WITH ALL FEATURES**\")\n",
        "        print(\"=\"*150)\n",
        "\n",
        "        # Display complete results for each horizon\n",
        "        for horizon in CONFIG['HORIZONS']:\n",
        "            horizon_signals = [s for s in all_signals if s['horizon'] == f'{horizon}d']\n",
        "            if horizon_signals:\n",
        "                df_horizon = pd.DataFrame(horizon_signals)\n",
        "                create_complete_playbook_tables(df_horizon, horizon)\n",
        "\n",
        "        # Executive Summary\n",
        "        print(\"\\n\" + \"=\"*150)\n",
        "        print(\"📊 **EXECUTIVE SUMMARY - COMPLETE ANALYSIS**\")\n",
        "        print(\"=\"*150)\n",
        "        print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
        "        print(f\"Total Stocks Analyzed: {len(set([s['Stock'] for s in all_signals]))}\")\n",
        "        print(f\"Total Signals Generated: {len(all_signals)}\")\n",
        "\n",
        "        # Filter strong signals\n",
        "        strong_signals = [s for s in all_signals\n",
        "                         if s['Accuracy'] >= 60 and s['Signal'] in ['BUY', 'STRONG BUY', 'SELL', 'STRONG SELL']]\n",
        "\n",
        "        if strong_signals:\n",
        "            print(f\"\\n📊 Strong Signals Summary:\")\n",
        "            print(f\"- Total Strong Signals: {len(strong_signals)}\")\n",
        "\n",
        "            # Signals with proprietary features in top 5\n",
        "            prop_signals = [s for s in strong_signals\n",
        "                           if s.get('feature_presence', {}).get('proprietary', 0) > 0]\n",
        "            print(f\"- Signals with Proprietary Features in Top 5: {len(prop_signals)} \"\n",
        "                  f\"({len(prop_signals)/len(strong_signals)*100:.1f}%)\")\n",
        "\n",
        "            # Top opportunities\n",
        "            buy_signals = [s for s in strong_signals if s['Signal'] in ['BUY', 'STRONG BUY']]\n",
        "            sell_signals = [s for s in strong_signals if s['Signal'] in ['SELL', 'STRONG SELL']]\n",
        "\n",
        "            if buy_signals:\n",
        "                print(\"\\n📈 **TOP BUY OPPORTUNITIES (sorted by VIX/Momentum combo):**\")\n",
        "\n",
        "                buy_df = pd.DataFrame(buy_signals)\n",
        "                buy_df['combo_score'] = buy_df['Momentum125'] - buy_df['VIX'] + buy_df['FNG']/2\n",
        "                top_buys = buy_df.nlargest(min(10, len(buy_df)), 'combo_score')\n",
        "\n",
        "                print(\"\\n{:<8} {:<8} {:<6} {:<7} {:<4} {:<4} {:<4} {:<7} {:<8} {:<15}\".format(\n",
        "                    \"Stock\", \"Horizon\", \"Acc%\", \"Sharpe\", \"VIX\", \"FNG\", \"RSI\", \"Mom125%\", \"AnnVol%\", \"Signal\"\n",
        "                ))\n",
        "                print(\"-\" * 100)\n",
        "\n",
        "                for _, signal in top_buys.iterrows():\n",
        "                    signal_type = \"Strong Buy\" if \"STRONG\" in signal['Signal'] else \"Buy\"\n",
        "                    print(f\"{signal['Stock']:<8} {signal['horizon']:<8} {signal['Accuracy']:>5.1f} \"\n",
        "                          f\"{signal['Sharpe']:>7.2f} {display_value(signal['VIX']):>4} \"\n",
        "                          f\"{display_value(signal['FNG']):>4} {display_value(signal['RSI']):>4} \"\n",
        "                          f\"{display_value(signal['Momentum125']):>7} \"\n",
        "                          f\"{display_value(signal['AnnVolatility']):>8} {signal_type:<15}\")\n",
        "\n",
        "        # Save results\n",
        "        if IN_COLAB:\n",
        "            output_prefix = '/content/drive/MyDrive/complete_proprietary_signals'\n",
        "        else:\n",
        "            output_prefix = 'complete_proprietary_signals'\n",
        "\n",
        "        save_complete_analysis_results(all_signals, ml_model, output_prefix)\n",
        "\n",
        "        # Final summary\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ANALYSIS COMPLETE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Print complete legend\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"📚 **COMPLETE LEGEND AND EXPLANATIONS**\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(\"\\n**Signal Types:**\")\n",
        "        print(\"📈 = Buy/Up  📉 = Sell/Down\")\n",
        "        print(\"STRONG BUY/SELL = High confidence signal with multiple confirmations\")\n",
        "        print(\"BUY/SELL = Standard signal meeting criteria\")\n",
        "        print(\"NEUTRAL = Insufficient confidence or conflicting signals\")\n",
        "\n",
        "        print(\"\\n**Performance Indicators:**\")\n",
        "        print(\"🟢 = Good  🟡 = Moderate  🔴 = Poor  ⚪ = Neutral\")\n",
        "\n",
        "        print(\"\\n**Metric Thresholds:**\")\n",
        "        print(\"Accuracy: 🟢 ≥65%  🟡 55-65%  🔴 <55%\")\n",
        "        print(\"Sharpe: 🟢 ≥1.0  🟡 0.5-1.0  🔴 <0.5\")\n",
        "        print(\"CAGR: 🟢 ≥20%  🟡 10-20%  🔴 <10%\")\n",
        "\n",
        "        print(\"\\n**Proprietary Indicators:**\")\n",
        "        print(\"VIX: 🟢 <15  🟡 15-30  🔴 >30 (Market volatility)\")\n",
        "        print(\"FNG: 🔴 <25 (ExFear)  🔴 25-40 (Fear)  ⚪ 40-60 (Neutral)  🟢 60-75 (Greed)  🟢 >75 (ExGreed)\")\n",
        "        print(\"RSI: <30 = Oversold, 30-70 = Normal, >70 = Overbought\")\n",
        "        print(\"Momentum125: 🔴 <-10%  ⚪ -10% to 10%  🟡 10-20%  🟢 >20%\")\n",
        "        print(\"AnnVolatility: 🟢 <20%  🟡 20-40%  🔴 >40%\")\n",
        "\n",
        "        print(\"\\n**Feature Type Codes:**\")\n",
        "        print(\"[M] = Macro  [P] = Proprietary  [T] = Technical\")\n",
        "        print(\"[I] = Interaction  [R] = Regime  [X] = Transformed\")\n",
        "\n",
        "        print(\"\\n**Missing Data:**\")\n",
        "        print(\"— = Data not available or could not be calculated\")\n",
        "\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "        print(\"End of Analysis\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "def save_complete_analysis_results(signals, ml_model, output_prefix):\n",
        "    \"\"\"Save complete analysis results with all features\"\"\"\n",
        "    if not signals:\n",
        "        logger.warning(\"No signals to save\")\n",
        "        return\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(signals)\n",
        "\n",
        "    # Save complete results to CSV\n",
        "    csv_filename = f\"{output_prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    logger.info(f\"Saved complete results to {csv_filename}\")\n",
        "\n",
        "    # Save enhanced summary to JSON\n",
        "    summary = {\n",
        "        'analysis_date': datetime.now().isoformat(),\n",
        "        'total_signals': len(signals),\n",
        "        'horizons': list(set([s['horizon'] for s in signals])),\n",
        "        'stocks_analyzed': list(set([s['ticker'] for s in signals])),\n",
        "        'signal_distribution': dict(pd.Series([s['signal'] for s in signals]).value_counts()),\n",
        "        'driver_distribution': dict(pd.Series([s['driver_type'] for s in signals]).value_counts()),\n",
        "        'proprietary_feature_statistics': {},\n",
        "        'feature_type_coverage': {}\n",
        "    }\n",
        "\n",
        "    # Add proprietary feature statistics\n",
        "    for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "        if feat in df.columns:\n",
        "            values = df[feat].dropna()\n",
        "            if len(values) > 0:\n",
        "                summary['proprietary_feature_statistics'][feat] = {\n",
        "                    'mean': float(values.mean()),\n",
        "                    'std': float(values.std()),\n",
        "                    'min': float(values.min()),\n",
        "                    'max': float(values.max()),\n",
        "                    'coverage': float(len(values) / len(df) * 100)\n",
        "                }\n",
        "\n",
        "    # Add feature type coverage\n",
        "    feature_type_counts = defaultdict(int)\n",
        "    for signal in signals:\n",
        "        if 'feature_presence' in signal:\n",
        "            for feat_type, count in signal['feature_presence'].items():\n",
        "                if count > 0:\n",
        "                    feature_type_counts[feat_type] += 1\n",
        "\n",
        "    for feat_type, count in feature_type_counts.items():\n",
        "        summary['feature_type_coverage'][feat_type] = {\n",
        "            'count': count,\n",
        "            'percentage': float(count / len(signals) * 100)\n",
        "        }\n",
        "\n",
        "    json_filename = f\"{output_prefix}_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(json_filename, 'w') as f:\n",
        "        json.dump(summary, f, indent=2, default=convert_np)\n",
        "    logger.info(f\"Saved complete summary to {json_filename}\")\n",
        "\n",
        "def calculate_indicators(stock_df):\n",
        "    \"\"\"Calculate ALL technical indicators for signal generation\"\"\"\n",
        "    indicators = {}\n",
        "\n",
        "    try:\n",
        "        # Get ALL proprietary features - either from data or calculate\n",
        "        prop_features = create_proprietary_features(stock_df)\n",
        "\n",
        "        # Add all proprietary features to indicators\n",
        "        for feat in CONFIG['PROPRIETARY_FEATURES']:\n",
        "            if feat in prop_features.columns:\n",
        "                value = prop_features[feat].iloc[-1]\n",
        "                indicators[feat] = float(value) if not pd.isna(value) else 0.0\n",
        "            else:\n",
        "                indicators[feat] = 0.0\n",
        "\n",
        "        # Add any additional calculated indicators\n",
        "        current_price = float(stock_df['Close'].iloc[-1])\n",
        "\n",
        "        # Moving averages\n",
        "        if len(stock_df) >= 50:\n",
        "            sma50 = stock_df['Close'].rolling(window=50).mean().iloc[-1]\n",
        "            indicators['price_sma50'] = float(current_price / sma50) if sma50 > 0 and not pd.isna(sma50) else 1.0\n",
        "        else:\n",
        "            indicators['price_sma50'] = 1.0\n",
        "\n",
        "        if len(stock_df) >= 200:\n",
        "            sma200 = stock_df['Close'].rolling(window=200).mean().iloc[-1]\n",
        "            indicators['price_sma200'] = float(current_price / sma200) if sma200 > 0 and not pd.isna(sma200) else 1.0\n",
        "        else:\n",
        "            indicators['price_sma200'] = 1.0\n",
        "\n",
        "        # Momentum\n",
        "        if len(stock_df) >= 20:\n",
        "            price_20d_ago = stock_df['Close'].iloc[-20]\n",
        "            indicators['momentum_20d'] = float((current_price / price_20d_ago - 1)) if price_20d_ago > 0 else 0.0\n",
        "        else:\n",
        "            indicators['momentum_20d'] = 0.0\n",
        "\n",
        "        # Bollinger Bands\n",
        "        bb_data = calculate_bollinger_bands(stock_df['Close'])\n",
        "        bb_percent_val = bb_data['percent_b'].iloc[-1]\n",
        "        indicators['bb_percent'] = float(bb_percent_val) if not pd.isna(bb_percent_val) else 0.5\n",
        "\n",
        "        # MACD components\n",
        "        macd_data = calculate_macd(stock_df['Close'])\n",
        "        indicators['macd_histogram'] = float(macd_data['histogram'].iloc[-1]) if not pd.isna(macd_data['histogram'].iloc[-1]) else 0.0\n",
        "        indicators['macd_bullish_cross'] = bool(macd_data['bullish_cross'])\n",
        "        indicators['macd_bearish_cross'] = bool(macd_data['bearish_cross'])\n",
        "\n",
        "        # Ensure all values are Python scalars\n",
        "        for key, value in indicators.items():\n",
        "            if hasattr(value, 'item'):\n",
        "                indicators[key] = value.item()\n",
        "            elif isinstance(value, (np.ndarray, pd.Series)):\n",
        "                if len(value) > 0:\n",
        "                    indicators[key] = float(value[0]) if not isinstance(value[0], bool) else bool(value[0])\n",
        "                else:\n",
        "                    indicators[key] = 0.0 if key not in ['macd_bullish_cross', 'macd_bearish_cross'] else False\n",
        "\n",
        "        logger.debug(f\"Calculated {len(indicators)} indicators including all proprietary features\")\n",
        "\n",
        "        return indicators\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in calculate_indicators: {str(e)}\")\n",
        "        # Return default values for all proprietary features\n",
        "        default_indicators = {feat: 0.0 for feat in CONFIG['PROPRIETARY_FEATURES']}\n",
        "        default_indicators.update({\n",
        "            'price_sma50': 1.0, 'price_sma200': 1.0, 'momentum_20d': 0.0,\n",
        "            'bb_percent': 0.5, 'macd_histogram': 0.0,\n",
        "            'macd_bullish_cross': False, 'macd_bearish_cross': False\n",
        "        })\n",
        "        return default_indicators\n",
        "\n",
        "def calculate_combination_confidence(prob_up, indicators, regime, top_features_by_type):\n",
        "    \"\"\"Calculate confidence based on feature combinations\"\"\"\n",
        "    confidence = 40.0\n",
        "\n",
        "    # 1. Model probability contribution (20 points max)\n",
        "    prob_up = ensure_scalar(prob_up)\n",
        "    prob_confidence = abs(prob_up - 0.5) * 40\n",
        "    confidence += prob_confidence * 0.5\n",
        "\n",
        "    # 2. Feature diversity scoring (40 points max)\n",
        "    diversity_score = 0\n",
        "\n",
        "    if top_features_by_type:\n",
        "        # Check how many different feature types are in top 5\n",
        "        feature_types_present = set()\n",
        "        for category in ['macro', 'proprietary', 'technical', 'interaction']:\n",
        "            if category in top_features_by_type and top_features_by_type[category]:\n",
        "                feature_types_present.add(category)\n",
        "\n",
        "        # More diverse features = higher confidence\n",
        "        diversity_score = len(feature_types_present) * 10\n",
        "        confidence += diversity_score\n",
        "\n",
        "    # 3. Critical combinations (40 points max)\n",
        "    combination_score = 0\n",
        "    combination_count = 0\n",
        "\n",
        "    # VIX-based combinations\n",
        "    vix = indicators.get('VIX', 20)\n",
        "    if vix > 30:\n",
        "        if prob_up < 0.45:\n",
        "            combination_score += 3  # High VIX + bearish prediction\n",
        "        else:\n",
        "            combination_score -= 1  # High VIX + bullish prediction (contrarian)\n",
        "        combination_count += 1\n",
        "    elif vix < 15:\n",
        "        if prob_up > 0.55:\n",
        "            combination_score += 3  # Low VIX + bullish prediction\n",
        "        combination_count += 1\n",
        "\n",
        "    # FNG-based combinations\n",
        "    fng = indicators.get('FNG', 50)\n",
        "    if fng < 25:\n",
        "        if indicators.get('RSI', 50) < 30:\n",
        "            combination_score += 2  # Fear + oversold\n",
        "        combination_count += 1\n",
        "    elif fng > 75:\n",
        "        if indicators.get('RSI', 50) > 70:\n",
        "            combination_score += 2  # Greed + overbought\n",
        "        combination_count += 1\n",
        "\n",
        "    # Momentum combinations\n",
        "    momentum = indicators.get('Momentum125', 0)\n",
        "    if momentum > 30:\n",
        "        if indicators.get('AnnVolatility', 30) < 25:\n",
        "            combination_score += 2  # Strong momentum + low volatility\n",
        "        combination_count += 1\n",
        "    elif momentum < -30:\n",
        "        if vix > 35:\n",
        "            combination_score += 2  # Negative momentum + high fear\n",
        "        combination_count += 1\n",
        "\n",
        "    if combination_count > 0:\n",
        "        confidence += (combination_score / combination_count) * 40\n",
        "\n",
        "    # Cap confidence\n",
        "    confidence = max(20, min(90, confidence))\n",
        "\n",
        "    return confidence\n",
        "\n",
        "def determine_combination_signal(prob_up, confidence, indicators, top_features_by_type):\n",
        "    \"\"\"Determine signal based on feature combinations and diversity\"\"\"\n",
        "    # Define thresholds\n",
        "    BUY_THRESHOLD = 0.55\n",
        "    SELL_THRESHOLD = 0.45\n",
        "    MIN_CONFIDENCE = 50\n",
        "\n",
        "    # Check confidence first\n",
        "    if confidence < MIN_CONFIDENCE:\n",
        "        return 'NEUTRAL'\n",
        "\n",
        "    # Extract key indicators\n",
        "    vix = indicators.get('VIX', 20)\n",
        "    fng = indicators.get('FNG', 50)\n",
        "    rsi = indicators.get('RSI', 50)\n",
        "    momentum = indicators.get('Momentum125', 0)\n",
        "    volatility = indicators.get('AnnVolatility', 30)\n",
        "    price_strength = indicators.get('PriceStrength', 0)\n",
        "    volume_breadth = indicators.get('VolumeBreadth', 1)\n",
        "\n",
        "    # Check feature diversity\n",
        "    has_proprietary = False\n",
        "    if top_features_by_type and 'proprietary' in top_features_by_type:\n",
        "        has_proprietary = len(top_features_by_type['proprietary']) > 0\n",
        "\n",
        "    # Strong combination signals\n",
        "    # 1. Extreme VIX conditions\n",
        "    if vix > 40 and prob_up < 0.4 and rsi > 60:\n",
        "        return 'STRONG SELL'\n",
        "    elif vix < 12 and momentum > 40 and prob_up > 0.65:\n",
        "        return 'STRONG BUY'\n",
        "\n",
        "    # 2. Fear/Greed extremes\n",
        "    elif fng < 20 and rsi < 30 and prob_up > 0.55:\n",
        "        return 'BUY'  # Contrarian buy\n",
        "    elif fng > 85 and rsi > 70 and prob_up < 0.45:\n",
        "        return 'SELL'  # Contrarian sell\n",
        "\n",
        "    # 3. Momentum + Volatility combinations\n",
        "    elif momentum > 30 and volatility < 20 and prob_up > 0.6:\n",
        "        return 'STRONG BUY'\n",
        "    elif momentum < -30 and volatility > 40 and prob_up < 0.4:\n",
        "        return 'STRONG SELL'\n",
        "\n",
        "    # 4. Volume breadth signals\n",
        "    elif volume_breadth > 2.0 and price_strength > 50 and prob_up > 0.55:\n",
        "        return 'BUY'\n",
        "    elif volume_breadth < 0.5 and price_strength < -25 and prob_up < 0.45:\n",
        "        return 'SELL'\n",
        "\n",
        "    # Regular signals with feature diversity check\n",
        "    elif prob_up > BUY_THRESHOLD and confidence > 60:\n",
        "        if has_proprietary or confidence > 70:\n",
        "            # Check supporting conditions\n",
        "            bullish_conditions = 0\n",
        "            if vix < 25: bullish_conditions += 1\n",
        "            if momentum > 10: bullish_conditions += 1\n",
        "            if rsi < 70: bullish_conditions += 1\n",
        "            if volatility < 35: bullish_conditions += 1\n",
        "            if volume_breadth > 1.0: bullish_conditions += 1\n",
        "\n",
        "            if bullish_conditions >= 3:\n",
        "                return 'STRONG BUY'\n",
        "            elif bullish_conditions >= 2:\n",
        "                return 'BUY'\n",
        "            else:\n",
        "                return 'NEUTRAL'\n",
        "        else:\n",
        "            return 'NEUTRAL'  # Need proprietary features for buy signal\n",
        "\n",
        "    elif prob_up < SELL_THRESHOLD and confidence > 60:\n",
        "        if has_proprietary or confidence > 70:\n",
        "            # Check supporting conditions\n",
        "            bearish_conditions = 0\n",
        "            if vix > 25: bearish_conditions += 1\n",
        "            if momentum < -10: bearish_conditions += 1\n",
        "            if rsi > 30: bearish_conditions += 1\n",
        "            if volatility > 35: bearish_conditions += 1\n",
        "            if volume_breadth < 1.0: bearish_conditions += 1\n",
        "\n",
        "            if bearish_conditions >= 3:\n",
        "                return 'STRONG SELL'\n",
        "            elif bearish_conditions >= 2:\n",
        "                return 'SELL'\n",
        "            else:\n",
        "                return 'NEUTRAL'\n",
        "        else:\n",
        "            return 'NEUTRAL'  # Need proprietary features for sell signal\n",
        "\n",
        "    # Default to neutral\n",
        "    else:\n",
        "        return 'NEUTRAL'\n",
        "\n",
        "def format_shap_feature_complete(feat_name, shap_val, feat_value, category):\n",
        "    \"\"\"Format SHAP feature with complete information\"\"\"\n",
        "    # Determine display name based on category and feature\n",
        "    if category == 'macro':\n",
        "        base_name = feat_name.replace('fred_', '').split('_')[0].upper()\n",
        "        if base_name in FRED_METADATA:\n",
        "            display_name = FRED_METADATA[base_name]['name']\n",
        "            if len(display_name) > 20:\n",
        "                display_name = base_name\n",
        "        else:\n",
        "            display_name = base_name\n",
        "    elif category == 'proprietary':\n",
        "        display_name = feat_name\n",
        "    elif category == 'interaction':\n",
        "        parts = feat_name.split('_X_')\n",
        "        if len(parts) >= 2:\n",
        "            macro_part = parts[0].replace('fred_', '').split('_')[0]\n",
        "            other_part = parts[1].split('_')[0]\n",
        "            display_name = f\"{macro_part}×{other_part}\"\n",
        "        else:\n",
        "            display_name = feat_name[:20]\n",
        "    elif category == 'transformed':\n",
        "        base_feat = feat_name.split('_')[0]\n",
        "        transform = feat_name.split('_')[-1]\n",
        "        display_name = f\"{base_feat}_{transform}\"\n",
        "    elif category == 'regime':\n",
        "        display_name = feat_name\n",
        "    else:  # technical\n",
        "        if 'sma' in feat_name:\n",
        "            display_name = feat_name.upper()\n",
        "        elif 'returns' in feat_name:\n",
        "            period = feat_name.split('_')[-1]\n",
        "            display_name = f\"Ret_{period}\"\n",
        "        else:\n",
        "            display_name = feat_name\n",
        "\n",
        "    # Format value\n",
        "    if isinstance(feat_value, (int, float)):\n",
        "        if abs(feat_value) >= 1000:\n",
        "            value_str = f\"{feat_value:.0f}\"\n",
        "        elif abs(feat_value) >= 10:\n",
        "            value_str = f\"{feat_value:.1f}\"\n",
        "        elif abs(feat_value) >= 1:\n",
        "            value_str = f\"{feat_value:.2f}\"\n",
        "        else:\n",
        "            value_str = f\"{feat_value:.3f}\"\n",
        "    else:\n",
        "        value_str = str(feat_value)[:10]\n",
        "\n",
        "    # Direction\n",
        "    direction = \"↑\" if shap_val > 0 else \"↓\"\n",
        "\n",
        "    # Category abbreviation\n",
        "    cat_abbrev = {\n",
        "        'macro': 'M',\n",
        "        'proprietary': 'P',\n",
        "        'technical': 'T',\n",
        "        'interaction': 'I',\n",
        "        'regime': 'R',\n",
        "        'transformed': 'X'\n",
        "    }.get(category, 'U')\n",
        "\n",
        "    return f\"[{cat_abbrev}] {display_name}={value_str} ({shap_val:+.3f}{direction})\"\n",
        "\n",
        "def label_fng(val: int) -> str:\n",
        "    \"\"\"Get textual label for FNG\"\"\"\n",
        "    if val >= 85:\n",
        "        return \"ExGreed\"\n",
        "    elif val >= 75:\n",
        "        return \"Greed\"\n",
        "    elif val >= 60:\n",
        "        return \"Greed+\"\n",
        "    elif val >= 40:\n",
        "        return \"Neutral\"\n",
        "    elif val >= 25:\n",
        "        return \"Fear\"\n",
        "    elif val >= 15:\n",
        "        return \"Fear+\"\n",
        "    else:\n",
        "        return \"ExFear\"\n",
        "\n",
        "def create_if_then_logic_complete(stock_name, horizon, direction, signal, accuracy,\n",
        "                                 shap_features, indicators, sharpe, current_price,\n",
        "                                 bl20, bh20, feature_presence):\n",
        "    \"\"\"Create IF/THEN logic with complete feature information\"\"\"\n",
        "\n",
        "    # Initialize the logic components\n",
        "    if_conditions = []\n",
        "\n",
        "    # Add top SHAP features with their types\n",
        "    feature_conditions = []\n",
        "    for feat in shap_features[:3]:  # Top 3 features\n",
        "        feat_name = feat['feature']\n",
        "        shap_val = feat['shap_value']\n",
        "        actual_val = feat.get('actual_value', 0)\n",
        "        feat_type = feat.get('feature_type', 'unknown')\n",
        "\n",
        "        # Format based on feature type\n",
        "        type_label = feat_type[0].upper()  # First letter of type\n",
        "\n",
        "        if feat_type == 'proprietary' and feat_name in ['VIX', 'FNG', 'RSI', 'Momentum125']:\n",
        "            if feat_name == 'VIX':\n",
        "                condition = f\"{feat_name}={actual_val:.0f}\"\n",
        "            elif feat_name == 'FNG':\n",
        "                condition = f\"{feat_name}={actual_val:.0f}({label_fng(actual_val)})\"\n",
        "            elif feat_name == 'Momentum125':\n",
        "                condition = f\"Mom125={actual_val:.0f}%\"\n",
        "            else:\n",
        "                condition = f\"{feat_name}={actual_val:.1f}\"\n",
        "        else:\n",
        "            condition = f\"{feat_name[:15]}={display_value(actual_val)}\"\n",
        "\n",
        "        feature_conditions.append(f\"[{type_label}]{condition}({shap_val:+.2f})\")\n",
        "\n",
        "    if feature_conditions:\n",
        "        if_conditions.append(\"Features: \" + \", \".join(feature_conditions))\n",
        "\n",
        "    # Add critical combinations\n",
        "    critical_combos = []\n",
        "\n",
        "    vix = indicators.get('VIX', 20)\n",
        "    fng = indicators.get('FNG', 50)\n",
        "    rsi = indicators.get('RSI', 50)\n",
        "    momentum = indicators.get('Momentum125', 0)\n",
        "    volatility = indicators.get('AnnVolatility', 30)\n",
        "\n",
        "    # VIX-based combinations\n",
        "    if vix > 30:\n",
        "        if rsi > 70:\n",
        "            critical_combos.append(f\"VIX>{30} & RSI>{70}\")\n",
        "        elif momentum < -20:\n",
        "            critical_combos.append(f\"VIX>{30} & Mom<-20%\")\n",
        "    elif vix < 15:\n",
        "        if momentum > 20:\n",
        "            critical_combos.append(f\"VIX<{15} & Mom>{20}%\")\n",
        "\n",
        "    # FNG-based combinations\n",
        "    if fng < 25 and rsi < 30:\n",
        "        critical_combos.append(f\"Fear({fng}) & RSI<{30}\")\n",
        "    elif fng > 75 and rsi > 70:\n",
        "        critical_combos.append(f\"Greed({fng}) & RSI>{70}\")\n",
        "\n",
        "    if critical_combos:\n",
        "        if_conditions.append(\"Combos: \" + \" | \".join(critical_combos))\n",
        "\n",
        "    # Add feature diversity information\n",
        "    if feature_presence:\n",
        "        diversity_str = \"Types: \" + \", \".join([f\"{k}({v})\" for k, v in feature_presence.items() if v > 0])\n",
        "        if_conditions.append(diversity_str)\n",
        "\n",
        "    # Create the IF/THEN statement\n",
        "    if_part = \" AND \".join(if_conditions) if if_conditions else \"No significant features\"\n",
        "\n",
        "    # Determine confidence level\n",
        "    if accuracy >= 70 and len(feature_presence) >= 3:\n",
        "        confidence_text = f\"{accuracy:.0f}% very high confidence (diverse signals)\"\n",
        "    elif accuracy >= 65:\n",
        "        confidence_text = f\"{accuracy:.0f}% high confidence\"\n",
        "    elif accuracy >= 55:\n",
        "        confidence_text = f\"{accuracy:.0f}% moderate confidence\"\n",
        "    else:\n",
        "        confidence_text = f\"{accuracy:.0f}% low confidence\"\n",
        "\n",
        "    # Build the complete logic string\n",
        "    logic = f\"{stock_name} ({horizon}d) {'↑' if direction == 'Up' else '↓'} \"\n",
        "    logic += f\"IF {if_part} \"\n",
        "    logic += f\"THEN {signal} ({confidence_text}). \"\n",
        "\n",
        "    # Add signal quality indicator\n",
        "    has_proprietary = feature_presence.get('proprietary', 0) > 0\n",
        "    has_macro = feature_presence.get('macro', 0) > 0\n",
        "    has_interaction = feature_presence.get('interaction', 0) > 0\n",
        "\n",
        "    if has_proprietary and (has_macro or has_interaction):\n",
        "        logic += \"High-quality mixed signal. ✅✅\"\n",
        "    elif has_proprietary:\n",
        "        logic += \"Proprietary-driven signal. ✅\"\n",
        "    elif len(feature_presence) >= 3:\n",
        "        logic += \"Diverse feature signal. ✅\"\n",
        "    else:\n",
        "        logic += \"Limited feature diversity. ⚠️\"\n",
        "\n",
        "    return logic\n",
        "\n",
        "# Run main if executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}